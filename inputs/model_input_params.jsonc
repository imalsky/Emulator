{
  // ==========================================================================
  // Enhanced Transformer Model Configuration
  // ==========================================================================
  // Configuration for predicting atmospheric variables and spectral sequences
  // using an advanced transformer architecture with global-local feature integration
  // ==========================================================================

  // ==========================================================================
  // DATA SPECIFICATION AND PREPROCESSING
  // ==========================================================================

  "model_type": "encoder",  
  
  // Variables specification - determines model input/output dimensions
  "input_variables": [
    "pressure",       // Atmospheric pressure profile [sequential]
    "temperature",    // Temperature profile [sequential]
    "orbital_sep"     // Orbital separation [global]
  ],
  "target_variables": [
    "thermal_net_flux",    // Thermal emission spectrum
    "reflected_net_flux"   // Reflected light spectrum
  ],
  
  // Data preprocessing
  "frac_of_data": 0.25,     // Fraction of dataset to use (useful for quick tests)
  "normalization": {
    // Variable-specific normalization methods
    "key_methods": {
      "pressure": "log-min-max",         // Log-scale normalization for pressure
      "thermal_net_flux": "custom",      // Custom scaling for thermal flux
      "reflected_net_flux": "custom"     // Custom scaling for reflected flux
    },
    "default_method": "iqr",             // Inter-quartile range normalization for other variables
    "clip_outliers_before_scaling": false // Whether to clip extreme values before normalization
  },

  // ==========================================================================
  // MODEL ARCHITECTURE 
  // ==========================================================================
  
  // Core architecture parameters
  "d_model": 256,             // Hidden dimension size (larger = more capacity, doubled from original)
  "nhead": 8,                 // Number of attention heads (doubled from original, must divide d_model evenly)
  "num_layers": 6,            // Number of transformer layers (increased from 4 for more depth)
  "dim_feedforward": 1024,    // Feedforward network size (typically 4x d_model)
  "mlp_layers": 3,            // Number of layers in output MLP
  "mlp_hidden_dim": 128,      // Hidden dimension in output MLP (doubled from original)
  
  // Feature integration and encoding
  "integration_method": "film", // How to combine global and sequential features:
                               // "add" - simple addition
                               // "film" - feature-wise linear modulation (better)
                               // "concat" - concatenation followed by projection
  "pos_encoding_type": "sine", // Positional encoding: "sine" (fixed) or "learned" (trainable)
  
  // Regularization parameters
  "norm_first": true,           // Use pre-norm architecture (generally more stable)
  "stochastic_depth_rate": 0.1, // Probability of dropping layers during training (modern regularization)
  "dropout": 0.1,               // Dropout probability (increased slightly from 0.05)
  "weight_decay": 1e-5,         // L2 regularization coefficient
  "activation": "gelu",         // Activation function: "gelu" is preferred over "relu" for transformers
  "layer_norm_eps": 1e-5,       // Epsilon for LayerNorm stability
  "max_sequence_length": 100,
  
  // Performance optimization
  "use_torch_compile": true,    // Use PyTorch 2.0+ compilation for speed
  "use_amp": true,              // Enable automatic mixed precision for memory/speed
  
  // ==========================================================================
  // TRAINING PARAMETERS
  // ==========================================================================
  
  // Basic training settings
  "batch_size": 16,                  // Increased batch size from 16 for better statistics
  "epochs": 100,                     // Increased maximum training epochs
  "num_workers": 4,                  // Number of worker processes for data loading
  "loss_function": "mse",            // Loss function: "mse", "l1", or "smooth_l1"
  "gradient_clip_val": 1.0,          // Maximum gradient norm for clipping
  
  // Optimizer settings
  "optimizer": "adamw",              // Optimizer type: "adamw", "adam", "sgd"
  "learning_rate": 5e-4,             // Slightly increased from 3e-4 for faster initial convergence
  "min_lr": 1e-8,                    // Minimum learning rate
  "gamma": 0.2,                      // Learning rate decay factor
  "lr_patience": 5,                  // Epochs to wait before reducing learning rate
  
  // Early stopping settings
  "early_stopping_patience": 20,     // Stop training after N epochs without improvement
  "min_delta": 1e-8,                 // Minimum improvement to reset patience counter
  
  // Reproducibility
  "random_seed": 42                  // Random seed for reproducible results
}
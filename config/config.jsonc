{
// ════════════════════════════════════════════════════════════════
// ║           ATMOSPHERIC PROFILE TRANSFORMER CONFIG             ║
// ║                    IMPROVED VERSION                          ║
// ║                                                              ║
// ║  PADDING & MASKING CONVENTION:                              ║
// ║  - Padding value: -9999.0 (unlikely in atmospheric data)    ║
// ║  - Mask convention: True = padding, False = valid           ║
// ║  - Follows PyTorch's key_padding_mask standard              ║
// ║  - Sequences are right-padded (padding at the end)          ║
// ║  - Loss computation excludes padding positions              ║
// ║  - Model outputs are NOT overwritten (industry standard)    ║
// ════════════════════════════════════════════════════════════════

// ════════════════════════════════════════════════════════════════
// ║                1.  GENERAL / MISCELLANEOUS                   ║
// ════════════════════════════════════════════════════════════════
// Settings that affect the whole pipeline.
"miscellaneous_settings": {
  // Reproducibility seed for Python, NumPy, Torch, etc.
  "random_seed": 42,                         // int ≥ 0
  
  // Data-loader worker processes (0 = main thread).
  // CHANGED: Increased for better GPU utilization
  "num_workers": 12,                         // int ≥ 0
  
  // Turn on PyTorch's anomaly detection (debug only; slows training).
  "detect_anomaly": false,                   // bool
  
  // Number of samples per NPY shard in processed data.
  "shard_size": 4096,                        // int ≥ 1
  
  // Enable torch.compile for model optimization (requires PyTorch 2.0+)
  "torch_compile": true,                     // bool
  
  // Compile mode if torch_compile is enabled
  // CHANGED: max-autotune for best performance on modern GPUs
  "compile_mode": "max-autotune",            // "default", "reduce-overhead", "max-autotune"
  
  // Enable torch.export for model export
  "torch_export": true                       // bool
},

// ════════════════════════════════════════════════════════════════
// ║                 2.  DATA PATHS & SPLITS                      ║
// ════════════════════════════════════════════════════════════════
"data_paths_config": {
  // HDF5 files sitting in data/raw/.
  // Multiple files supported; order doesn't matter.
  "hdf5_dataset_filename": [
    "picaso_results_1.h5",
    "picaso_results_2.h5",
    "picaso_results_3.h5",
    "picaso_results_4.h5",
    "picaso_results_5.h5",
    "picaso_results_6.h5",
    "picaso_results_7.h5",
    "picaso_results_8.h5",
    "picaso_results_9.h5",
    "picaso_results_10.h5"
  ],

  // Optional pre-made JSON of train/val/test indices.
  // If missing, code auto-generates new splits.
  "dataset_splits_filename": "dataset_splits.json"
},

// ════════════════════════════════════════════════════════════════
// ║                 3.  DATA SPECIFICATION                       ║
// ════════════════════════════════════════════════════════════════
"data_specification": {
  // Per-layer sequence inputs (T×features).
  // These are the atmospheric profile features at each altitude level.
  "input_variables": [
    "pressure_bar",      // Atmospheric pressure in bars
    "temperature_k"      // Temperature in Kelvin
  ],

  // Optional per-profile scalars (FiLM conditioning).
  // These are global properties that modulate the entire profile.
  "global_variables": [
    "orbital_distance_m" // Distance from star in meters
  ],

  // Targets (the model predicts these).
  // These are the quantities we want to predict from the inputs.
  "target_variables": [
    "net_thermal_flux",   // Thermal radiation flux
    "net_reflected_flux"  // Reflected radiation flux
  ],

  // Padding value for variable-length sequences.
  // CHANGED: Fixed to match codebase standard (-9999.0 instead of -1e30)
  // IMPORTANT: This value is used as a sentinel to identify padding positions.
  // - Must be a value that never occurs naturally in your data
  // - All features at a padded timestep will be set to this value
  // - The loss function will automatically exclude these positions
  // - Model outputs at padding positions are NOT overwritten (industry standard)
  "padding_value": -9999.0                  // float
},

// ════════════════════════════════════════════════════════════════
// ║                 4.  NORMALIZATION                            ║
// ════════════════════════════════════════════════════════════════
"normalization": {
  // Per-key normalization method (falls back to default_method if unspecified).
  // Methods available:
  // - "standard": Zero mean, unit variance
  // - "log-standard": Log transform then standardize
  // - "symlog": Symmetric log (linear near zero, log elsewhere)
  // - "log-min-max": Log transform then min-max scaling
  // - "iqr": Interquartile range normalization
  // - "max-out": Scale by maximum absolute value
  // - "signed-log": Signed log transform
  // - "scaled_signed_offset_log": Scaled signed log with offset
  // - "bool": Binary values (no normalization)
  // - "none": No normalization
  "key_methods": {
    "pressure_bar":       "log-min-max",    // Pressure spans orders of magnitude
    "temperature_k":      "standard",       // Temperature is roughly normal
    "orbital_distance_m": "log-standard",   // Distance spans orders of magnitude
    "net_thermal_flux":   "symlog",         // Flux can be positive/negative with wide range
    "net_reflected_flux": "symlog"          // Flux can be positive/negative with wide range
  },

  // Fallback for any variable missing above.
  "default_method": "standard",

  // Numerical epsilon for log operations (prevents log(0)).
  "epsilon": 1e-9,                          // float > 0

  // Max values kept in RAM for quantile/IQR computations.
  "quantile_max_values_in_memory": 10000000, // int ≥ 1

  // Percentile that defines linear→log switch in symlog.
  // CHANGED: Increased for better handling of near-zero values
  "symlog_percentile": 0.5                  // 0 ≤ float ≤ 1
},

// ════════════════════════════════════════════════════════════════
// ║                 5.  MODEL HYPERPARAMETERS                    ║
// ════════════════════════════════════════════════════════════════
"model_hyperparameters": {
  // Hidden dimension of transformer embeddings.
  // CHANGED: Increased from 256 to 512 for better capacity
  "d_model": 512,                            // int, power of 2 recommended
  
  // Attention heads (must divide d_model exactly).
  // CHANGED: Increased to 8 (512/8 = 64 dims per head, standard)
  "nhead": 8,                                // int
  
  // Number of transformer encoder layers.
  // CHANGED: Increased from 3 to 6 for deeper model
  "num_encoder_layers": 6,                   // int
  
  // FFN inner dimension (typically 4×d_model).
  // CHANGED: Set to 4×d_model (2048) following transformer standard
  "dim_feedforward": 2048,                   // int
  
  // Dropout applied inside encoder & projection layers.
  // CHANGED: Set to 0.1 for regularization (0.0 can overfit)
  "dropout": 0.1,                            // 0-1 float
  
  // NEW: Separate dropout for attention layers
  // Often beneficial to have different dropout for attention
  "attention_dropout": 0.1,                  // 0-1 float
  
  // NEW: FiLM layer clamping value
  // Controls how much global features can modulate the model
  "film_clamp": 2.0,                         // float > 0 or null for no clamping
  
  // Maximum sequence length your data can reach.
  // Sequences longer than this will be truncated.
  // Sequences shorter than this will be right-padded.
  "max_sequence_length": 256                 // int ≥ max observed seq len
},

// ════════════════════════════════════════════════════════════════
// ║                 6.  TRAINING HYPERPARAMETERS                 ║
// ════════════════════════════════════════════════════════════════
"training_hyperparameters": {
  // Maximum number of training epochs.
  // CHANGED: Increased to 300 for better convergence
  "epochs": 300,                             // int ≥ 1
  
  // Mini-batch size per GPU.
  // CHANGED: Reduced from 512 to 256 for stability with larger model
  "batch_size": 256,                         // int ≥ 1
  
  // Initial learning rate.
  // CHANGED: Increased to 2e-4 for faster initial learning
  "learning_rate": 2e-4,                     // float > 0
  
  // L2 weight decay (regularization).
  // CHANGED: Reduced slightly for less regularization
  "weight_decay": 5e-6,                      // float ≥ 0
  
  // Optimizer: choose "adamw", "adam", or "sgd".
  "optimizer": "adamw",
  
  // Gradient clipping value (0 disables).
  "gradient_clip_val": 1.0,                  // float ≥ 0
  
  // Gradient accumulation steps for large effective batch.
  // CHANGED: Set to 2 for effective batch size of 512
  "gradient_accumulation_steps": 2,          // int ≥ 1

  // ─── Learning Rate Scheduler ─────────────────────────────────
  // "reduce_on_plateau": Reduce LR when validation loss plateaus
  // "cosine": Cosine annealing with optional warmup
  "scheduler_type": "cosine",

  // Parameters for ReduceLROnPlateau (if selected)
  "lr_patience": 10,                         // epochs without improvement
  "lr_factor": 0.5,                          // new_lr = old_lr * factor

  // Parameters for CosineAnnealingLR (if selected)
  // CHANGED: Increased warmup for stability with larger model
  "warmup_epochs": 10,                       // linear warmup before cosine

  // Minimum learning rate for any scheduler.
  "min_lr": 1e-8,

  // ─── Early Stopping ──────────────────────────────────────────
  // CHANGED: Increased patience for better convergence
  "early_stopping_patience": 30,             // epochs without improvement
  // CHANGED: More sensitive to improvements
  "min_delta": 1e-6,                         // minimum improvement threshold

  // ─── Mixed Precision & Data Fraction ─────────────────────────
  "use_amp": true,                           // Automatic Mixed Precision (faster on modern GPUs)
  
  // Fraction of dataset to use (for quick experiments).
  "dataset_fraction_to_use": 1.0,            // 0-1 float

  // Validation/Test split fractions (if splits file absent).
  "val_frac": 0.15,                          // 15% for validation
  "test_frac": 0.15,                         // 15% for test

  // Abort epoch if too many batches are all padding.
  "max_batch_failure_rate": 0.10             // 0-1 float
},

// ════════════════════════════════════════════════════════════════
// ║                 7.  OUTPUT / SAVING                          ║
// ════════════════════════════════════════════════════════════════
"output_paths_config": {
  // Folder name under models/ where checkpoints & logs go.
  "fixed_model_foldername": "trained_model"
},

// ════════════════════════════════════════════════════════════════
// ║                 8.  HYPERPARAMETER SEARCH SPACE              ║
// ════════════════════════════════════════════════════════════════
// Bounds for Optuna hyperparameter optimization.
// CHANGED: Updated ranges based on improved model architecture
"hyperparameter_search": {
  // Categorical choices for transformer architecture
  // CHANGED: Larger model dimensions for better capacity
  "d_model": [256, 384, 512, 768],           // Model dimensions to try
  "nhead_divisors": [4, 6, 8, 12, 16],       // nhead must divide d_model
  // CHANGED: Wider range of depths
  "num_encoder_layers": [4, 10],             // Range of layer counts
  // CHANGED: Removed fixed values, will use 4×d_model
  "dim_feedforward": null,                   // Will be set to 4×d_model

  // Continuous ranges (Optuna will sample from these)
  "dropout": {
    "low": 0.0,
    "high": 0.2                              // CHANGED: Increased upper bound
  },
  // NEW: Separate attention dropout search
  "attention_dropout": {
    "low": 0.0,
    "high": 0.2
  },
  "learning_rate": {
    "low": 5e-5,                             // CHANGED: Wider range
    "high": 5e-4,
    "log": true                              // Sample in log space
  },
  "weight_decay": {
    "low": 1e-7,                             // CHANGED: Lower minimum
    "high": 1e-4,
    "log": true                              // Sample in log space
  },
  // NEW: FiLM clamping search
  "film_clamp": {
    "low": 1.0,
    "high": 5.0
  },

  // Batch sizes to try
  // CHANGED: Added more options
  "batch_size": [64, 128, 256, 384, 512],
  
  // NEW: Gradient accumulation search
  "gradient_accumulation_steps": [1, 2, 4]
}
}
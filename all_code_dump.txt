===== /Users/imalsky/Desktop/Problemulator/src/preprocess.py =====
#!/usr/bin/env python3
"""
preprocess.py - Preprocess raw HDF5 data into normalized NPY shards.

This module handles:
- Loading raw atmospheric profile data from HDF5 files
- Applying normalization using computed statistics
- Padding sequences to maximum length
- Saving processed data as NPY shards for efficient loading
"""
from __future__ import annotations

import datetime
import logging
import time
from pathlib import Path
from typing import Any, Dict, List, Tuple, Optional

import h5py
import numpy as np
import torch
from tqdm import tqdm

from normalizer import DataNormalizer
from utils import compute_data_hash_with_stats, ensure_dirs, save_json

logger = logging.getLogger(__name__)

# Processing constants
HDF5_READ_CHUNK_SIZE = 131072


def _group_indices_by_file(indices: List[Tuple[str, int]]) -> Dict[str, List[int]]:
    """
    Group indices by file stem for efficient batch loading.
    
    Args:
        indices: List of (file_stem, index) tuples
        
    Returns:
        Dictionary mapping file_stem to list of indices
    """
    grouped = {}
    for file_stem, idx in indices:
        grouped.setdefault(file_stem, []).append(idx)
    return grouped


def _load_and_restore_chunk(
    hf_file: h5py.File,
    variables: List[str],
    indices: np.ndarray,
    max_seq_len: Optional[int] = None
) -> Optional[Dict[str, np.ndarray]]:
    """
    Load a chunk of data for specified variables and indices.
    
    Uses sort/unsort technique for efficient HDF5 reading while preserving order.
    
    Args:
        hf_file: Open HDF5 file handle
        variables: List of variable names to load
        indices: Array of indices to load
        max_seq_len: Optional maximum sequence length for truncation
        
    Returns:
        Dictionary of variable_name -> data array, or None if error
    """
    if not indices.size:
        return {}
    
    # Sort indices for efficient HDF5 reading
    sorter = np.argsort(indices)
    sorted_indices = indices[sorter]
    
    # Compute inverse permutation to restore original order
    inverse_sorter = np.argsort(sorter)
    
    data_chunk = {}
    for var in variables:
        if var not in hf_file:
            logger.error(f"Critical error: Variable '{var}' not found in HDF5 file.")
            return None
        
        # Load data in sorted order (efficient for HDF5)
        data_sorted = hf_file[var][sorted_indices]
        
        # Restore original order
        data_orig_order = data_sorted[inverse_sorter]
        
        # Apply truncation if needed
        if max_seq_len is not None and data_orig_order.ndim == 2:
            if data_orig_order.shape[1] > max_seq_len:
                logger.warning(
                    f"Truncating sequence from {data_orig_order.shape[1]} to {max_seq_len} "
                    f"for variable '{var}'"
                )
                data_orig_order = data_orig_order[:, :max_seq_len]
        
        data_chunk[var] = data_orig_order
    
    return data_chunk


def _save_shard(
    shard_buffers: Dict[str, List[np.ndarray]],
    seq_dir: Path,
    tgt_dir: Path,
    glb_dir: Optional[Path],
    shard_idx: int,
) -> None:
    """
    Save a shard of data to NPY files.
    
    Args:
        shard_buffers: Dictionary containing buffered data arrays
        seq_dir: Directory for sequence input shards
        tgt_dir: Directory for target shards
        glb_dir: Directory for global feature shards (optional)
        shard_idx: Index of this shard
    """
    # Skip empty shards
    if (
        not shard_buffers["sequence_inputs"]
        or not shard_buffers["sequence_inputs"][0].size
    ):
        return
    
    shard_name = f"shard_{shard_idx:06d}.npy"
    
    # Save sequence inputs
    seq_data = np.concatenate(shard_buffers["sequence_inputs"], axis=0)
    np.save(seq_dir / shard_name, seq_data)
    
    # Save targets
    tgt_data = np.concatenate(shard_buffers["targets"], axis=0)
    np.save(tgt_dir / shard_name, tgt_data)
    
    # Save globals if present
    if (
        glb_dir is not None
        and shard_buffers.get("globals")
        and shard_buffers["globals"]
    ):
        glb_data = np.concatenate(shard_buffers["globals"], axis=0)
        np.save(glb_dir / shard_name, glb_data)


def _save_preprocessing_summary(
    output_dir: Path,
    config: Dict[str, Any],
    all_splits: Dict[str, list],
    norm_metadata: Dict[str, Any],
    data_hash: str,
) -> None:
    """
    Save a human-readable summary of preprocessing results.
    
    Args:
        output_dir: Output directory for summary
        config: Configuration dictionary
        all_splits: Dictionary of data splits
        norm_metadata: Normalization metadata
        data_hash: Hash of data configuration
    """
    summary_path = output_dir / "preprocessing_summary.txt"
    logger.info(f"Saving preprocessing summary to {summary_path}")
    
    try:
        # Build summary content
        lines = [
            "# Preprocessing Summary",
            f"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"Data Hash: {data_hash}",
            "",
            "--- Data Specification ---",
            f"Input Variables: {config['data_specification']['input_variables']}",
            f"Global Variables: {config['data_specification'].get('global_variables', 'None')}",
            f"Target Variables: {config['data_specification']['target_variables']}",
            f"Padding Value: {config['data_specification']['padding_value']}",
            f"Max Sequence Length: {config['model_hyperparameters']['max_sequence_length']}",
            "",
            "--- Data Splits ---",
        ]
        
        shard_size = config.get("miscellaneous_settings", {}).get("shard_size", 1000)
        for name, indices in all_splits.items():
            count = len(indices)
            num_shards = (count + shard_size - 1) // shard_size if count > 0 else 0
            lines.extend([
                f"{name.capitalize()} Split:",
                f"  Total Samples: {count:,}",
                f"  Shards Created: {num_shards}",
            ])
        
        lines.extend(["", "--- Normalization Metadata ---"])
        methods = norm_metadata.get("normalization_methods", {})
        for var, stats in norm_metadata.get("per_key_stats", {}).items():
            lines.extend([
                f"Variable: '{var}'",
                f"  Method: {methods.get(var, 'N/A')}",
            ])
        
        content = "\n".join(lines)
        summary_path.write_text(content)
        
    except Exception as e:
        logger.error(f"Could not write preprocessing summary: {e}")


def preprocess_data(
    config: Dict[str, Any],
    raw_hdf5_paths: List[Path],
    splits: Dict[str, List[Tuple[str, int]]],
    processed_dir: Path,
) -> bool:
    """
    Main preprocessing pipeline.
    
    Orchestrates:
    1. Computing normalization statistics from training data
    2. Loading raw data in chunks
    3. Applying normalization
    4. Padding sequences
    5. Saving as NPY shards
    
    Args:
        config: Configuration dictionary
        raw_hdf5_paths: List of raw HDF5 file paths
        splits: Dictionary of train/val/test splits
        processed_dir: Output directory for processed data
        
    Returns:
        True if successful, False otherwise
    """
    ensure_dirs(processed_dir)
    
    # Compute hash for cache invalidation
    current_hash = compute_data_hash_with_stats(config, raw_hdf5_paths)
    hash_path = processed_dir / "data_hash.txt"
    metadata_path = processed_dir / "normalization_metadata.json"
    
    shard_size = config.get("miscellaneous_settings", {}).get("shard_size", 4096)
    all_splits = {
        "train": splits["train"],
        "val": splits["validation"],
        "test": splits["test"],
    }
    
    # Check if preprocessing is needed
    if (
        hash_path.exists()
        and hash_path.read_text().strip() == current_hash
        and metadata_path.exists()
    ):
        logger.info(
            "Processed data is up-to-date based on configuration and file stats. "
            "Skipping preprocessing."
        )
        return True
    
    logger.info("Configuration or source files have changed. Starting preprocessing...")
    preprocessing_start_time = time.time()
    
    # Step 1: Calculate normalization statistics
    start_time = time.time()
    normalizer = DataNormalizer(config_data=config)
    norm_metadata = normalizer.calculate_stats(raw_hdf5_paths, splits["train"])
    
    if not save_json(norm_metadata, metadata_path):
        logger.error("Failed to save normalization metadata.")
        return False
    
    logger.info(
        f"Normalization metadata computed and saved in {time.time() - start_time:.2f}s."
    )
    
    # Prepare for processing
    file_map = {path.stem: path for path in raw_hdf5_paths if path.is_file()}
    data_spec = config["data_specification"]
    input_vars = data_spec["input_variables"]
    global_vars = data_spec.get("global_variables", [])
    target_vars = data_spec["target_variables"]
    padding_value = data_spec["padding_value"]
    max_seq_len = config["model_hyperparameters"]["max_sequence_length"]
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Using device: {device} for normalization.")
    
    # Step 2: Process each split
    for split_name, split_indices in all_splits.items():
        if not split_indices:
            logger.warning(f"No indices for '{split_name}' split. Skipping.")
            continue
        
        split_start_time = time.time()
        grouped_indices = _group_indices_by_file(split_indices)
        
        # Create output directories
        split_dir = processed_dir / split_name
        seq_dir = split_dir / "sequence_inputs"
        tgt_dir = split_dir / "targets"
        glb_dir = split_dir / "globals" if global_vars else None
        ensure_dirs(seq_dir, tgt_dir, glb_dir)
        
        # Save split metadata
        num_shards = (len(split_indices) + shard_size - 1) // shard_size
        split_metadata = {
            "total_samples": len(split_indices),
            "shard_size": shard_size,
            "num_shards": num_shards,
            "sequence_length": max_seq_len,
            "has_globals": bool(global_vars),
        }
        save_json(split_metadata, split_dir / "metadata.json")
        
        # Initialize shard buffers
        shard_buffers = {
            "sequence_inputs": [],
            "targets": [],
            "globals": [] if global_vars else None,
        }
        num_samples_in_buffer = 0
        current_shard_idx = 0
        
        logger.info(f"Processing {split_name} split ({len(split_indices)} profiles)...")
        
        with tqdm(total=len(split_indices), desc=f"Processing {split_name}") as pbar:
            for file_stem, indices in grouped_indices.items():
                if file_stem not in file_map:
                    logger.warning(f"Skipping unknown file '{file_stem}'.")
                    pbar.update(len(indices))
                    continue
                
                with h5py.File(file_map[file_stem], "r") as hf_raw:
                    # Process in chunks for memory efficiency
                    for i in range(0, len(indices), HDF5_READ_CHUNK_SIZE):
                        chunk_idx = np.array(indices[i:i + HDF5_READ_CHUNK_SIZE])
                        
                        # Load data
                        input_data = _load_and_restore_chunk(
                            hf_raw, input_vars, chunk_idx, max_seq_len
                        )
                        target_data = _load_and_restore_chunk(
                            hf_raw, target_vars, chunk_idx, max_seq_len
                        )
                        global_data = (
                            _load_and_restore_chunk(hf_raw, global_vars, chunk_idx)
                            if global_vars
                            else {}
                        )
                        
                        # Check for errors
                        if (
                            input_data is None
                            or target_data is None
                            or (global_vars and global_data is None)
                        ):
                            logger.error(
                                "A required variable was missing from HDF5 file. Aborting."
                            )
                            return False
                        
                        # Stack variables into arrays
                        seq_in_np = np.stack(
                            [input_data[var] for var in input_vars], axis=-1
                        )
                        tgt_np = np.stack(
                            [target_data[var] for var in target_vars], axis=-1
                        )
                        glb_np = (
                            np.stack([global_data[var] for var in global_vars], axis=-1)
                            if global_vars
                            else None
                        )
                        
                        # Convert to tensors for normalization
                        seq_in = torch.from_numpy(seq_in_np).to(device).float()
                        tgt = torch.from_numpy(tgt_np).to(device).float()
                        glb = (
                            torch.from_numpy(glb_np).to(device).float()
                            if global_vars and glb_np is not None
                            else None
                        )
                        
                        # Apply normalization to each variable
                        for j, var in enumerate(input_vars):
                            method = normalizer.key_methods.get(var, "none")
                            stats = norm_metadata.get("per_key_stats", {}).get(var)
                            if stats:
                                seq_in[:, :, j] = normalizer.normalize_tensor(
                                    seq_in[:, :, j], method, stats
                                )
                        
                        for j, var in enumerate(target_vars):
                            method = normalizer.key_methods.get(var, "none")
                            stats = norm_metadata.get("per_key_stats", {}).get(var)
                            if stats:
                                tgt[:, :, j] = normalizer.normalize_tensor(
                                    tgt[:, :, j], method, stats
                                )
                        
                        if global_vars and glb is not None:
                            for j, var in enumerate(global_vars):
                                method = normalizer.key_methods.get(var, "none")
                                stats = norm_metadata.get("per_key_stats", {}).get(var)
                                if stats:
                                    glb[:, j] = normalizer.normalize_tensor(
                                        glb[:, j], method, stats
                                    )
                        
                        # Apply padding if sequences are shorter than max_seq_len
                        current_seq_len = seq_in.shape[1]
                        pad_width = max_seq_len - current_seq_len
                        
                        if pad_width > 0:
                            # Pad sequences and targets
                            pad_spec = ((0, 0), (0, pad_width), (0, 0))
                            seq_in_np = np.pad(
                                seq_in.cpu().numpy(),
                                pad_spec,
                                constant_values=padding_value,
                            )
                            tgt_np = np.pad(
                                tgt.cpu().numpy(),
                                pad_spec,
                                constant_values=padding_value,
                            )
                        else:
                            seq_in_np = seq_in.cpu().numpy()
                            tgt_np = tgt.cpu().numpy()
                        
                        glb_np = (
                            glb.cpu().numpy()
                            if global_vars and glb is not None
                            else None
                        )
                        
                        # Add to shard buffers
                        shard_buffers["sequence_inputs"].append(
                            seq_in_np.astype(np.float32)
                        )
                        shard_buffers["targets"].append(tgt_np.astype(np.float32))
                        
                        if global_vars and glb_np is not None:
                            shard_buffers["globals"].append(glb_np.astype(np.float32))
                        
                        num_samples_in_buffer += len(chunk_idx)
                        pbar.update(len(chunk_idx))
                        
                        # Save complete shards
                        while num_samples_in_buffer >= shard_size:
                            # Concatenate all buffered data
                            full_seq_data = np.concatenate(
                                shard_buffers["sequence_inputs"], axis=0
                            )
                            full_tgt_data = np.concatenate(
                                shard_buffers["targets"], axis=0
                            )
                            
                            # Extract one shard worth of data
                            seq_to_save = full_seq_data[:shard_size]
                            tgt_to_save = full_tgt_data[:shard_size]
                            
                            temp_shard_buffer = {
                                "sequence_inputs": [seq_to_save],
                                "targets": [tgt_to_save],
                            }
                            
                            # Handle globals if present
                            if global_vars and shard_buffers.get("globals"):
                                full_glb_data = np.concatenate(
                                    shard_buffers["globals"], axis=0
                                )
                                glb_to_save = full_glb_data[:shard_size]
                                temp_shard_buffer["globals"] = [glb_to_save]
                                
                                # Keep remainder for next shard
                                shard_buffers["globals"] = (
                                    [full_glb_data[shard_size:]]
                                    if full_glb_data.shape[0] > shard_size
                                    else []
                                )
                            
                            # Save the shard
                            _save_shard(
                                temp_shard_buffer,
                                seq_dir,
                                tgt_dir,
                                glb_dir,
                                current_shard_idx,
                            )
                            current_shard_idx += 1
                            
                            # Keep remainder for next shard
                            shard_buffers["sequence_inputs"] = (
                                [full_seq_data[shard_size:]]
                                if full_seq_data.shape[0] > shard_size
                                else []
                            )
                            shard_buffers["targets"] = (
                                [full_tgt_data[shard_size:]]
                                if full_tgt_data.shape[0] > shard_size
                                else []
                            )
                            
                            num_samples_in_buffer -= shard_size
        
        # Save any remaining samples as final partial shard
        if num_samples_in_buffer > 0:
            logger.info(
                f"Saving final shard for '{split_name}' with {num_samples_in_buffer} samples."
            )
            _save_shard(
                shard_buffers,
                seq_dir,
                tgt_dir,
                glb_dir,
                current_shard_idx
            )
        
        logger.info(
            f"Completed {split_name} split in {time.time() - split_start_time:.2f}s."
        )
    
    # Save summary and hash
    _save_preprocessing_summary(
        processed_dir, config, all_splits, norm_metadata, current_hash
    )
    hash_path.write_text(current_hash)
    
    total_time = time.time() - preprocessing_start_time
    logger.info(f"Preprocessing completed successfully in {total_time:.2f}s.")
    
    return True


__all__ = ["preprocess_data"]

===== /Users/imalsky/Desktop/Problemulator/src/model.py =====
#!/usr/bin/env python3
"""
model.py - Optimized transformer model with FiLM conditioning and export compatibility.

Architecture:
- Transformer encoder with sinusoidal positional encoding
- FiLM (Feature-wise Linear Modulation) for global context conditioning
- Export-friendly implementation avoiding dynamic control flow
- Support for torch.compile and torch.export

PADDING CONVENTION:
- Mask values: True = padding position, False = valid position
- This follows PyTorch's convention for src_key_padding_mask
- Padding positions are excluded from attention and loss computation
"""
from __future__ import annotations

import logging
import math
from pathlib import Path
from typing import Any, Dict, Optional, Union

import torch
import torch.nn as nn
from packaging import version
from torch import Tensor
from torch.export import Dim, export as texport, save as tsave

from utils import DTYPE, PADDING_VALUE, validate_config

logger = logging.getLogger(__name__)


class SinePositionalEncoding(nn.Module):
    """
    Sinusoidal positional encoding for sequential data.
    
    Caches encodings by sequence length for efficiency and export compatibility.
    """
    
    def __init__(self, d_model: int) -> None:
        """
        Initialize positional encoding.
        
        Args:
            d_model: Model dimension (must be even)
        """
        super().__init__()
        self.d_model = d_model
        
        # Pre-register buffer for better export compatibility
        self.register_buffer('_cached_pe', None, persistent=False)
        self._cached_seq_len = -1
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Add positional encoding to input tensor.
        
        Args:
            x: Input tensor of shape (batch_size, seq_len, d_model)
            
        Returns:
            Tensor with positional encoding added
        """
        batch_size, seq_len, d_model = x.shape
        
        # Cache positional encoding for efficiency
        if self._cached_pe is None or self._cached_seq_len != seq_len:
            # Generate position indices
            position = torch.arange(seq_len, device=x.device, dtype=DTYPE).unsqueeze(1)
            
            # Generate frequency terms
            div_term = torch.exp(
                torch.arange(0, self.d_model, 2, device=x.device, dtype=DTYPE) *
                (-math.log(10000.0) / self.d_model)
            )
            
            # Create positional encoding
            pe = torch.zeros(1, seq_len, self.d_model, device=x.device, dtype=DTYPE)
            pe[0, :, 0::2] = torch.sin(position * div_term)
            pe[0, :, 1::2] = torch.cos(position * div_term)
            
            # Cache for reuse - FIXED: use direct assignment, not register_buffer
            self._cached_pe = pe
            self._cached_seq_len = seq_len
        
        return x + self._cached_pe


class FiLMLayer(nn.Module):
    """
    Feature-wise Linear Modulation layer.
    
    Modulates features using global context through learned scale and shift.
    Uses near-identity initialization for stable training.
    """
    
    def __init__(
        self,
        context_dim: int,
        feature_dim: int,
        clamp_gamma: Optional[float] = 1.0
    ) -> None:
        """
        Initialize FiLM layer.
        
        Args:
            context_dim: Dimension of context vector
            feature_dim: Dimension of features to modulate
            clamp_gamma: Maximum magnitude for scale/shift (None for no clamping)
        """
        super().__init__()
        
        # Project context to scale and shift parameters
        self.projection = nn.Linear(context_dim, feature_dim * 2)
        
        # Near-identity initialization for stable training
        nn.init.normal_(self.projection.weight, mean=0.0, std=0.01)
        nn.init.zeros_(self.projection.bias)
        
        # Store clamp value as buffer for export compatibility
        if clamp_gamma is not None:
            self.register_buffer('clamp_value', torch.tensor(clamp_gamma))
        else:
            self.register_buffer('clamp_value', torch.tensor(float('inf')))
    
    def forward(self, features: Tensor, context: Tensor) -> Tensor:
        """
        Apply FiLM modulation to features.
        
        Args:
            features: Features to modulate (batch, seq_len, feature_dim)
            context: Global context (batch, context_dim)
            
        Returns:
            Modulated features
        """
        # Get scale and shift parameters
        gamma_beta = self.projection(context)
        delta_gamma, beta = torch.chunk(gamma_beta, 2, dim=-1)
        
        # Apply clamping (becomes no-op with inf clamp_value)
        delta_gamma = torch.clamp(delta_gamma, -self.clamp_value, self.clamp_value)
        beta = torch.clamp(beta, -self.clamp_value, self.clamp_value)
        
        # Expand for sequence dimension
        delta_gamma = delta_gamma.unsqueeze(1)
        beta = beta.unsqueeze(1)
        
        # Apply FiLM transformation: y = x * (1 + γ) + β
        return features * (1.0 + delta_gamma) + beta


class DecomposedTransformerEncoderLayer(nn.Module):
    """
    Custom transformer encoder layer built from primitives.
    
    Fully compatible with torch.export and torch.compile.
    Supports both pre-norm and post-norm architectures.
    
    ATTENTION MASKING:
    - Uses src_key_padding_mask where True = padding position
    - Padding positions are prevented from attending or being attended to
    """
    
    def __init__(
        self,
        d_model: int,
        nhead: int,
        dim_feedforward: int = 2048,
        dropout: float = 0.1,
        activation: str = "gelu",
        norm_first: bool = True,
        batch_first: bool = True,
    ) -> None:
        """
        Initialize transformer encoder layer.
        
        Args:
            d_model: Model dimension
            nhead: Number of attention heads
            dim_feedforward: Feedforward network dimension
            dropout: Dropout probability
            activation: Activation function ("gelu" or "relu")
            norm_first: If True, use pre-norm architecture
            batch_first: If True, expect batch dimension first
        """
        super().__init__()
        
        self.d_model = d_model
        self.nhead = nhead
        self.norm_first = norm_first
        
        # Multi-head attention
        self.self_attn = nn.MultiheadAttention(
            d_model,
            nhead,
            dropout=dropout,
            batch_first=batch_first
        )
        
        # Feed-forward network
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        
        # Activation function
        if activation == "gelu":
            self.activation = nn.GELU()
        elif activation == "relu":
            self.activation = nn.ReLU()
        else:
            raise ValueError(f"Unsupported activation: {activation}")
        
        # Normalization layers
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # Dropout layers
        self.dropout = nn.Dropout(dropout)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self) -> None:
        """Initialize weights following PyTorch's TransformerEncoderLayer."""
        nn.init.xavier_uniform_(self.linear1.weight)
        nn.init.xavier_uniform_(self.linear2.weight)
        nn.init.constant_(self.linear1.bias, 0)
        nn.init.constant_(self.linear2.bias, 0)
    
    def forward(
        self,
        src: Tensor,
        src_mask: Optional[Tensor] = None,
        src_key_padding_mask: Optional[Tensor] = None,
    ) -> Tensor:
        """
        Forward pass through transformer encoder layer.
        
        Args:
            src: Source sequence (batch, seq_len, d_model)
            src_mask: Attention mask (optional)
            src_key_padding_mask: Padding mask where True = padding position
            
        Returns:
            Transformed sequence
        """
        if self.norm_first:
            # Pre-norm architecture
            x = src
            
            # Self-attention block
            x2 = self.norm1(x)
            x2, _ = self.self_attn(
                x2, x2, x2,
                attn_mask=src_mask,
                key_padding_mask=src_key_padding_mask,  # True = padding
                need_weights=False
            )
            x = x + self.dropout1(x2)
            
            # Feed-forward block
            x2 = self.norm2(x)
            x2 = self.linear2(self.dropout(self.activation(self.linear1(x2))))
            x = x + self.dropout2(x2)
        else:
            # Post-norm architecture
            x = src
            
            # Self-attention block
            x2, _ = self.self_attn(
                x, x, x,
                attn_mask=src_mask,
                key_padding_mask=src_key_padding_mask,  # True = padding
                need_weights=False
            )
            x = x + self.dropout1(x2)
            x = self.norm1(x)
            
            # Feed-forward block
            x2 = self.linear2(self.dropout(self.activation(self.linear1(x))))
            x = x + self.dropout2(x2)
            x = self.norm2(x)
        
        return x


class TransformerBlock(nn.Module):
    """
    Combined transformer + optional FiLM block.
    
    Avoids dynamic control flow for export compatibility.
    """
    
    def __init__(
        self,
        d_model: int,
        nhead: int,
        dim_feedforward: int,
        dropout: float,
        global_input_dim: int,
        film_clamp: Optional[float] = 1.0,
    ):
        """
        Initialize transformer block with optional FiLM.
        
        Args:
            d_model: Model dimension
            nhead: Number of attention heads
            dim_feedforward: Feedforward dimension
            dropout: Dropout probability
            global_input_dim: Dimension of global features (0 for no FiLM)
            film_clamp: Clamping value for FiLM parameters
        """
        super().__init__()
        
        self.has_film = global_input_dim > 0
        
        # Transformer layer
        self.transformer = DecomposedTransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation="gelu",
            norm_first=True,
            batch_first=True,
        )
        
        # Optional FiLM layer
        if self.has_film:
            self.film = FiLMLayer(global_input_dim, d_model, clamp_gamma=film_clamp)
    
    def forward(
        self,
        x: Tensor,
        global_features: Optional[Tensor] = None,
        src_key_padding_mask: Optional[Tensor] = None,
    ) -> Tensor:
        """
        Forward pass through transformer block.
        
        Args:
            x: Input sequence
            global_features: Optional global context
            src_key_padding_mask: Padding mask (True = padding)
            
        Returns:
            Transformed sequence
        """
        # Always pass through transformer
        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)
        
        # Apply FiLM if it exists and global features are provided
        if self.has_film and global_features is not None:
            x = self.film(x, global_features)
        
        return x


class PredictionModel(nn.Module):
    """
    Transformer model for atmospheric profile regression.
    
    Features:
    - Deep transformer architecture with FiLM conditioning
    - Sinusoidal positional encoding
    - Export-friendly implementation
    - Proper padding mask handling (no output overwriting)
    
    IMPORTANT: This model does NOT overwrite outputs at padding positions.
    The loss function handles masking, following industry standards.
    """
    
    def __init__(
        self,
        input_dim: int,
        global_input_dim: int,
        output_dim: int,
        d_model: int = 256,
        nhead: int = 8,
        num_encoder_layers: int = 6,
        dim_feedforward: int = 1024,
        dropout: float = 0.1,
        padding_value: float = PADDING_VALUE,
        film_clamp: Optional[float] = 1.0,
    ) -> None:
        """
        Initialize prediction model.
        
        Args:
            input_dim: Dimension of input features
            global_input_dim: Dimension of global features (0 for none)
            output_dim: Dimension of output predictions
            d_model: Model dimension
            nhead: Number of attention heads
            num_encoder_layers: Number of transformer layers
            dim_feedforward: Feedforward dimension
            dropout: Dropout probability
            padding_value: Value used for padding (for reference only)
            film_clamp: Clamping value for FiLM parameters
        """
        super().__init__()
        
        if d_model % nhead != 0:
            raise ValueError(f"d_model ({d_model}) must be divisible by nhead ({nhead})")
        
        self.d_model = d_model
        self.has_global_features = global_input_dim > 0
        
        # Note: We store padding_value for reference but don't use it for output masking
        self.padding_value = padding_value
        
        # Input projection
        self.input_proj = nn.Sequential(
            nn.Linear(input_dim, d_model),
            nn.LayerNorm(d_model),
            nn.GELU()
        )
        
        # Positional encoding
        self.pos_encoder = SinePositionalEncoding(d_model)
        
        # Initial FiLM if we have global features
        if self.has_global_features:
            self.initial_film = FiLMLayer(global_input_dim, d_model, clamp_gamma=film_clamp)
        else:
            self.initial_film = None
        
        # Build transformer blocks with integrated FiLM
        self.blocks = nn.ModuleList()
        for _ in range(num_encoder_layers):
            self.blocks.append(
                TransformerBlock(
                    d_model=d_model,
                    nhead=nhead,
                    dim_feedforward=dim_feedforward,
                    dropout=dropout,
                    global_input_dim=global_input_dim,
                    film_clamp=film_clamp,
                )
            )
        
        # Output projection
        self.output_proj = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, output_dim),
        )
        
        # Initialize weights
        self._init_weights()
        
        # Log model statistics
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        logger.info(
            f"PredictionModel created with {trainable_params:,} trainable parameters. "
            f"Architecture: d_model={d_model}, nhead={nhead}, layers={num_encoder_layers}"
        )
    
    def _init_weights(self) -> None:
        """Initialize weights for layers except FiLM and Transformer blocks."""
        for module in self.modules():
            if isinstance(module, (FiLMLayer, DecomposedTransformerEncoderLayer, TransformerBlock)):
                continue  # These have their own initialization
            elif isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode="fan_in", nonlinearity="relu")
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d, nn.BatchNorm2d)):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(
        self,
        sequence: Tensor,
        global_features: Optional[Tensor] = None,
        sequence_mask: Optional[Tensor] = None,
    ) -> Tensor:
        """
        Forward pass through the model.
        
        Args:
            sequence: Input sequence (batch, seq_len, input_dim)
            global_features: Optional global features (batch, global_dim)
            sequence_mask: Padding mask where True = padding position
            
        Returns:
            Output predictions (batch, seq_len, output_dim)
            
        Note: Outputs at padding positions are NOT overwritten.
              The loss function handles masking these positions.
        """
        # Project input features to model dimension
        x = self.input_proj(sequence)
        
        # Add positional encoding
        x = self.pos_encoder(x)
        
        # Apply initial FiLM if it exists
        if self.initial_film is not None and global_features is not None:
            x = self.initial_film(x, global_features)
        
        # Pass through transformer blocks
        for block in self.blocks:
            x = block(x, global_features, sequence_mask)
        
        # Project to output dimension
        output = self.output_proj(x)
        
        # IMPORTANT: We do NOT mask the output here.
        # The loss function handles masking, following industry standards.
        # This allows the model to produce predictions for all positions,
        # which can be useful for analysis, while the loss correctly
        # ignores padding positions during training.
        
        return output


def create_prediction_model(
    config: Dict[str, Any],
    device: Optional[torch.device] = None,
    compile_model: bool = True,
) -> PredictionModel:
    """
    Create a prediction model from configuration.
    
    Args:
        config: Configuration dictionary
        device: Device to place model on
        compile_model: Whether to apply torch.compile
        
    Returns:
        Initialized PredictionModel
    """
    validate_config(config)
    
    data_spec = config["data_specification"]
    model_params = config["model_hyperparameters"]
    
    if device is None:
        device = torch.device("cpu")
    
    # Create model
    model = PredictionModel(
        input_dim=len(data_spec["input_variables"]),
        global_input_dim=len(data_spec.get("global_variables", [])),
        output_dim=len(data_spec["target_variables"]),
        d_model=model_params.get("d_model", 256),
        nhead=model_params.get("nhead", 8),
        num_encoder_layers=model_params.get("num_encoder_layers", 6),
        dim_feedforward=model_params.get("dim_feedforward", 1024),
        dropout=float(model_params.get("dropout", 0.1)),
        padding_value=float(data_spec.get("padding_value", PADDING_VALUE)),
        film_clamp=1.0,
    )
    
    model.to(device=device)
    
    # Conditionally compile the model
    if compile_model:
        compile_enabled = config.get("miscellaneous_settings", {}).get("torch_compile", False)
        compile_mode = config.get("miscellaneous_settings", {}).get("compile_mode", "default")
        
        if (
            version.parse(torch.__version__) >= version.parse("2.0.0")
            and device.type == "cuda"
            and compile_enabled
        ):
            try:
                logger.info(f"Attempting torch.compile with mode='{compile_mode}'")
                
                # Compilation kwargs
                compile_kwargs = {"mode": compile_mode}
                
                # Use fullgraph for max optimization if no dynamic control flow
                if compile_mode == "max-autotune":
                    compile_kwargs["fullgraph"] = True
                
                model = torch.compile(model, **compile_kwargs)
                logger.info("Model compiled successfully")
                
            except Exception as e:
                logger.warning(f"torch.compile failed: {e}. Proceeding without compilation.")
        elif compile_enabled and device.type != "cuda":
            logger.info("torch.compile is only enabled for CUDA devices.")
    
    logger.info(f"Model moved to device: {device}")
    return model


def export_model(
    model: nn.Module,
    example_input: Dict[str, Tensor],
    save_path: Union[str, Path],
    config: Optional[Dict[str, Any]] = None,
) -> None:
    """
    Export model with torch.export for deployment.
    
    Performs validation to ensure exported model produces identical results.
    
    Args:
        model: Model to export
        example_input: Example input dictionary
        save_path: Path to save exported model
        config: Optional configuration dictionary
    """
    save_path = Path(save_path)
    save_dir = save_path.parent
    model_name = save_path.stem
    
    # Check if export is enabled
    if config is not None:
        export_enabled = config.get("miscellaneous_settings", {}).get("torch_export", True)
        if not export_enabled:
            logger.info("Model export disabled in config - skipping.")
            return
    
    model.eval()
    
    # Get original device
    original_device = next(model.parameters()).device
    
    # Unwrap torch.compile wrapper if present
    if hasattr(model, "_orig_mod"):
        logger.info("Extracting original model from compiled wrapper")
        model = model._orig_mod
    
    # Move to CPU for export (avoids device-specific issues)
    model = model.to('cpu')
    
    # Move example inputs to CPU
    sequence = example_input["sequence"].to('cpu')
    global_features = example_input.get("global_features")
    if global_features is not None:
        global_features = global_features.to('cpu')
    sequence_mask = example_input.get("sequence_mask")
    if sequence_mask is not None:
        sequence_mask = sequence_mask.to('cpu')
    
    # Ensure batch size > 1 for dynamic shapes
    batch_size = sequence.shape[0]
    if batch_size == 1:
        # Duplicate inputs for batch size 2
        export_sequence = torch.cat([sequence, sequence], dim=0)
        export_global = (
            torch.cat([global_features, global_features], dim=0)
            if global_features is not None else None
        )
        export_mask = (
            torch.cat([sequence_mask, sequence_mask], dim=0)
            if sequence_mask is not None else None
        )
    else:
        export_sequence = sequence
        export_global = global_features
        export_mask = sequence_mask
    
    # Prepare kwargs for export
    kwargs: Dict[str, Tensor] = {"sequence": export_sequence}
    if export_global is not None:
        kwargs["global_features"] = export_global
    if export_mask is not None:
        kwargs["sequence_mask"] = export_mask
    
    # Define dynamic shapes (batch dimension only)
    batch_dim = Dim("batch", min=1, max=1024)
    
    dynamic_shapes: Dict[str, Any] = {
        "sequence": {0: batch_dim}
    }
    if export_global is not None:
        dynamic_shapes["global_features"] = {0: batch_dim}
    if export_mask is not None:
        dynamic_shapes["sequence_mask"] = {0: batch_dim}
    
    try:
        # Export with torch.export
        with torch.no_grad():
            exported_program = texport(
                model,
                args=(),
                kwargs=kwargs,
                dynamic_shapes=dynamic_shapes,
                strict=False
            )
        
        # Save exported model
        export_path = save_dir / f"{model_name}_exported.pt2"
        tsave(exported_program, str(export_path))
        logger.info(f"Model exported successfully to {export_path}")
        
        # Validate exported model
        logger.info("Validating exported model...")
        with torch.no_grad():
            # Test with original batch size
            test_kwargs = {
                "sequence": sequence,
                "global_features": global_features,
                "sequence_mask": sequence_mask
            }
            test_kwargs = {k: v for k, v in test_kwargs.items() if v is not None}
            
            original_output = model(**test_kwargs)
            exported_output = exported_program.module()(**test_kwargs)
            
            if not torch.allclose(original_output, exported_output, rtol=1e-4, atol=1e-5):
                logger.warning("Exported model output differs from original!")
                max_diff = torch.max(torch.abs(original_output - exported_output)).item()
                logger.warning(f"Maximum difference: {max_diff}")
            else:
                logger.info("✓ Exported model validation passed")
                
    except Exception as exc:
        logger.error(f"Model export failed: {exc}", exc_info=True)
        
        # Try fallback without dynamic shapes
        logger.info("Attempting fallback export with static shapes...")
        try:
            with torch.no_grad():
                static_exported = texport(
                    model,
                    args=(),
                    kwargs=kwargs,
                    strict=False
                )
            static_path = save_dir / f"{model_name}_exported_static.pt2"
            tsave(static_exported, str(static_path))
            logger.warning(f"Static shape export succeeded: {static_path}")
        except Exception as fallback_exc:
            logger.error(f"Fallback export also failed: {fallback_exc}")
    
    finally:
        # Restore to original device if needed
        if original_device.type != 'cpu':
            model.to(original_device)


__all__ = ["PredictionModel", "create_prediction_model", "export_model"]

===== /Users/imalsky/Desktop/Problemulator/src/dataset.py =====
#!/usr/bin/env python3
"""
dataset.py - Optimized data loader with intelligent memory management.

Features:
- Automatic RAM vs disk-cache decision based on available memory
- Memory mapping for large files
- LRU cache for frequently accessed shards
- Safe padding detection with epsilon tolerance

PADDING CONVENTION:
- Padding value: -9999.0 (defined in config)
- Mask convention: True = padding position, False = valid position
- This follows PyTorch's convention for key_padding_mask
- All features at a timestep must equal padding_value for it to be considered padding
"""
from __future__ import annotations

import json
import logging
import psutil
from functools import partial
from pathlib import Path
from typing import Any, Callable, Dict, List, Tuple

import numpy as np
import torch
from torch import Tensor
from torch.utils.data import Dataset

from utils import DTYPE, PADDING_VALUE

logger = logging.getLogger(__name__)


class AtmosphericDataset(Dataset):
    """
    Dataset for loading preprocessed atmospheric profile data.
    
    Automatically chooses between RAM loading and disk caching based on
    available memory. Uses memory mapping for large files to reduce I/O.
    
    Padding Convention:
    - Sequences are right-padded (padding at the end)
    - Padding positions have all features set to padding_value
    - This is enforced during preprocessing
    """
    
    def __init__(
        self,
        dir_path: Path,
        config: Dict[str, Any],
        indices: List[int],
        force_disk_loading: bool = False,
    ) -> None:
        """
        Initialize dataset with automatic memory management.
        
        Args:
            dir_path: Directory containing processed data shards
            config: Configuration dictionary
            indices: List of sample indices to use (0-based within split)
            force_disk_loading: If True, force disk-based loading
        """
        super().__init__()
        
        if not dir_path.is_dir():
            raise RuntimeError(f"Directory not found: {dir_path}")
        
        self.dir_path = dir_path
        self.config = config
        # Keep the *requested* indices for reporting, but load using validated indices
        self.indices = list(indices)
        self.force_disk_loading = force_disk_loading
        
        # Extract data specification
        data_spec = self.config["data_specification"]
        self.input_variables = data_spec["input_variables"]
        self.target_variables = data_spec["target_variables"]
        self.global_variables = data_spec.get("global_variables", [])
        
        # Padding configuration for safe comparison
        self.padding_value = float(data_spec.get("padding_value", PADDING_VALUE))
        self.padding_epsilon = 1e-6  # Tolerance for floating-point comparison
        
        # Validate directory structure
        self._validate_structure()
        
        # Load shard metadata
        self._load_metadata()
        
        # Decide loading strategy and load data
        self._estimate_memory_and_load()
        
        logger.info(
            f"AtmosphericDataset initialized: {len(self)} samples from {dir_path} "
            f"(mode: {'RAM' if self.ram_mode else 'disk cache'})"
        )
        logger.info(f"Padding value: {self.padding_value}, epsilon: {self.padding_epsilon}")
    
    def _validate_structure(self) -> None:
        """Validate that required directories exist."""
        required_dirs = ["sequence_inputs", "targets"]
        if self.global_variables:
            required_dirs.append("globals")
        
        missing = [d for d in required_dirs if not (self.dir_path / d).exists()]
        if missing:
            raise RuntimeError(f"Missing directories in {self.dir_path}: {missing}")
    
    def _load_metadata(self) -> None:
        """Load metadata about shard structure."""
        metadata_path = self.dir_path / "metadata.json"
        if not metadata_path.exists():
            raise RuntimeError(f"Missing metadata.json in {self.dir_path}")
        
        with open(metadata_path, "r") as f:
            self.metadata = json.load(f)
        
        self.shard_size = self.metadata["shard_size"]
        self.total_samples = self.metadata["total_samples"]
        self.num_shards = self.metadata["num_shards"]
        self.has_globals = self.metadata["has_globals"]
        self.sequence_length = self.metadata["sequence_length"]
    
    def _estimate_memory_and_load(self) -> None:
        """
        Estimate memory requirements and choose loading strategy.
        
        If dataset fits in available RAM, load everything for fast access.
        Otherwise, use disk caching with memory mapping.
        """
        # Get available shard files
        seq_dir = self.dir_path / "sequence_inputs"
        self.seq_shards = sorted(seq_dir.glob("shard_*.npy"))
        
        if len(self.seq_shards) == 0:
            raise RuntimeError(f"No shard files found in {seq_dir}")
        
        # Estimate memory requirements
        seq_shard = np.load(self.seq_shards[0], mmap_mode='r')
        seq_bytes_per_sample = seq_shard.itemsize * int(np.prod(seq_shard.shape[1:]))
        
        tgt_path0 = self.dir_path / "targets" / self.seq_shards[0].name
        if not tgt_path0.exists():
            raise RuntimeError(f"Missing target shard file: {tgt_path0}")
        tgt_shard = np.load(tgt_path0, mmap_mode='r')
        tgt_bytes_per_sample = tgt_shard.itemsize * int(np.prod(tgt_shard.shape[1:]))
        
        total_bytes_per_sample = seq_bytes_per_sample + tgt_bytes_per_sample
        
        # Add globals if present
        if self.has_globals:
            glb_path0 = self.dir_path / "globals" / self.seq_shards[0].name
            if not glb_path0.exists():
                raise RuntimeError(f"Missing globals shard file: {glb_path0}")
            glb_shard = np.load(glb_path0, mmap_mode='r')
            glb_bytes_per_sample = glb_shard.itemsize * int(glb_shard.shape[1])
            total_bytes_per_sample += glb_bytes_per_sample
        
        # Validate and filter indices against total_samples for *both* modes
        self._valid_indices = [i for i in self.indices if 0 <= i < self.total_samples]
        n_invalid = len(self.indices) - len(self._valid_indices)
        if n_invalid:
            logger.warning(f"{n_invalid} indices out of range [0,{self.total_samples}) were dropped.")
        
        # Calculate total memory needed
        total_bytes_needed = total_bytes_per_sample * len(self._valid_indices)
        total_gb_needed = total_bytes_needed / (1024**3)
        
        # Get available memory (use 80% as safety margin)
        available_memory = psutil.virtual_memory().available
        available_gb = available_memory / (1024**3)
        safe_available_gb = available_gb * 0.8
        
        logger.info(
            f"Memory estimate: {total_gb_needed:.2f} GB needed for {len(self._valid_indices)} samples, "
            f"{safe_available_gb:.2f} GB safely available"
        )
        
        # Choose loading strategy
        if self.force_disk_loading:
            logger.info("Force disk loading enabled - using disk cache mode")
            self._setup_disk_cache()
            self.ram_mode = False
        elif total_gb_needed < safe_available_gb:
            logger.info("Loading entire dataset into RAM...")
            self._load_all_to_ram()
            self.ram_mode = True
        else:
            logger.warning(
                f"Dataset too large for RAM ({total_gb_needed:.2f} GB > "
                f"{safe_available_gb:.2f} GB available). Using disk cache mode."
            )
            self._setup_disk_cache()
            self.ram_mode = False
    
    def _load_all_to_ram(self) -> None:
        """Load entire dataset into RAM for fast access."""
        n_samples = len(self._valid_indices)
        
        # Early exit if nothing to load
        if n_samples == 0:
            self.ram_sequences = np.array([])
            self.ram_targets = np.array([])
            self.ram_globals = np.array([]) if self.has_globals else None
            self.ram_index_map = {}
            return
        
        # Allocate arrays
        seq_shape = (n_samples, self.sequence_length, len(self.input_variables))
        tgt_shape = (n_samples, self.sequence_length, len(self.target_variables))
        
        self.ram_sequences = np.zeros(seq_shape, dtype=np.float32)
        self.ram_targets = np.zeros(tgt_shape, dtype=np.float32)
        
        if self.has_globals:
            glb_shape = (n_samples, len(self.global_variables))
            self.ram_globals = np.zeros(glb_shape, dtype=np.float32)
        
        # Create index mapping
        self.ram_index_map = {}
        
        # Group validated indices by shard for efficient loading
        indices_by_shard = {}
        for idx, original_idx in enumerate(self._valid_indices):
            shard_idx = original_idx // self.shard_size
            within_shard_idx = original_idx % self.shard_size
            
            if shard_idx not in indices_by_shard:
                indices_by_shard[shard_idx] = []
            indices_by_shard[shard_idx].append((idx, within_shard_idx, original_idx))
        
        # Load data shard by shard
        processed = 0
        for shard_idx in sorted(indices_by_shard.keys()):
            shard_name = f"shard_{shard_idx:06d}.npy"
            
            # Load entire shard at once
            seq_data = np.load(self.dir_path / "sequence_inputs" / shard_name)
            tgt_data = np.load(self.dir_path / "targets" / shard_name)
            
            if self.has_globals:
                glb_data = np.load(self.dir_path / "globals" / shard_name)
            
            # Extract samples from this shard
            for idx, within_shard_idx, original_idx in indices_by_shard[shard_idx]:
                self.ram_sequences[idx] = seq_data[within_shard_idx]
                self.ram_targets[idx] = tgt_data[within_shard_idx]
                
                if self.has_globals:
                    self.ram_globals[idx] = glb_data[within_shard_idx]
                
                self.ram_index_map[idx] = original_idx
                processed += 1
            
            if processed % 10000 == 0 or processed == n_samples:
                logger.info(f"Loaded {processed}/{n_samples} samples into RAM")
    
    def _setup_disk_cache(self) -> None:
        """Setup disk caching with memory mapping for large files."""
        # Map validated indices to shards
        self.effective_indices = []
        for idx in self._valid_indices:
            shard_idx = idx // self.shard_size
            within_shard_idx = idx % self.shard_size
            self.effective_indices.append((idx, shard_idx, within_shard_idx))
        
        # Initialize caches
        self._shard_cache = {}  # For small shards loaded fully
        self._mmap_cache = {}   # For memory-mapped large shards
        self._cache_size = min(len(self.seq_shards), 200)
        self._cache_order = []
        
        # Pre-load frequently accessed shards
        logger.info(f"Pre-loading up to {self._cache_size} shards into cache...")
        unique_shard_indices = sorted(set(idx[1] for idx in self.effective_indices))
        
        for i, shard_idx in enumerate(unique_shard_indices[:self._cache_size]):
            self._load_shard(shard_idx)
            if (i + 1) % 20 == 0:
                logger.info(f"Pre-loaded {i + 1} shards")
    
    def _load_shard(self, shard_idx: int) -> Dict[str, np.ndarray]:
        """
        Load a shard with intelligent caching strategy.
        
        Small shards are loaded fully into memory with LRU eviction.
        Large shards are memory-mapped for efficient access.
        
        Args:
            shard_idx: Index of the shard to load
            
        Returns:
            Dictionary containing shard data
        """
        # Check if already in memory cache
        if shard_idx in self._shard_cache:
            # Move to end of LRU list
            self._cache_order.remove(shard_idx)
            self._cache_order.append(shard_idx)
            return self._shard_cache[shard_idx]
        
        # Check if already memory-mapped
        if shard_idx in self._mmap_cache:
            return self._mmap_cache[shard_idx]
        
        # Load shard files
        shard_name = f"shard_{shard_idx:06d}.npy"
        seq_path = self.dir_path / "sequence_inputs" / shard_name
        tgt_path = self.dir_path / "targets" / shard_name
        
        if not seq_path.exists() or not tgt_path.exists():
            raise RuntimeError(f"Missing shard files for shard {shard_idx}")
        
        # Decide loading strategy based on file size
        seq_size = seq_path.stat().st_size
        use_mmap = seq_size > 50 * 1024 * 1024  # Memory map if > 50MB
        
        if use_mmap:
            # Use memory mapping for large files
            shard_data = {
                "sequence_inputs": np.load(seq_path, mmap_mode='r'),
                "targets": np.load(tgt_path, mmap_mode='r'),
            }
            
            if self.has_globals:
                glb_path = self.dir_path / "globals" / shard_name
                if glb_path.exists():
                    shard_data["globals"] = np.load(glb_path, mmap_mode='r')
                else:
                    raise RuntimeError(f"Missing globals shard for shard {shard_idx}")
            
            # Store in mmap cache (no LRU needed as these are lightweight)
            self._mmap_cache[shard_idx] = shard_data
            
        else:
            # Load small files fully into memory
            shard_data = {
                "sequence_inputs": np.load(seq_path),
                "targets": np.load(tgt_path),
            }
            
            if self.has_globals:
                glb_path = self.dir_path / "globals" / shard_name
                if glb_path.exists():
                    shard_data["globals"] = np.load(glb_path)
                else:
                    raise RuntimeError(f"Missing globals shard for shard {shard_idx}")
            
            # Add to cache with LRU eviction
            if len(self._cache_order) >= self._cache_size:
                # Evict oldest
                oldest = self._cache_order.pop(0)
                del self._shard_cache[oldest]
            
            self._shard_cache[shard_idx] = shard_data
            self._cache_order.append(shard_idx)
        
        return shard_data
    
    def __len__(self) -> int:
        """Return number of loadable samples in dataset."""
        if getattr(self, "ram_mode", False):
            return len(self.ram_sequences) if hasattr(self, "ram_sequences") else 0
        return len(self.effective_indices) if hasattr(self, "effective_indices") else 0
    
    def __getitem__(self, idx: int) -> Tuple[Dict[str, Tensor], Tensor]:
        """
        Get a single sample from the dataset.
        
        Args:
            idx: Sample index
            
        Returns:
            Tuple of (input_dict, target_tensor)
        """
        if not (0 <= idx < len(self)):
            raise IndexError(f"Index {idx} out of range for dataset of size {len(self)}")
        
        if self.ram_mode:
            # Fast path: direct RAM access
            seq_in_np = self.ram_sequences[idx]
            tgt_np = self.ram_targets[idx]
            
            # Create input dictionary
            inputs = {"sequence": torch.from_numpy(seq_in_np.copy()).to(DTYPE)}
            
            if self.has_globals:
                glb_np = self.ram_globals[idx]
                inputs["global_features"] = torch.from_numpy(glb_np.copy()).to(DTYPE)
            
            targets = torch.from_numpy(tgt_np.copy()).to(DTYPE)
            
        else:
            # Disk cache path
            global_idx, shard_idx, within_shard_idx = self.effective_indices[idx]
            
            # Load the shard
            shard_data = self._load_shard(shard_idx)
            
            # Extract the specific sample
            seq_in_np = shard_data["sequence_inputs"][within_shard_idx]
            tgt_np = shard_data["targets"][within_shard_idx]
            
            # Copy from memory-mapped arrays to ensure tensor owns memory
            if hasattr(seq_in_np, 'base') and seq_in_np.base is not None:
                seq_in_np = seq_in_np.copy()
                tgt_np = tgt_np.copy()
            
            inputs = {"sequence": torch.from_numpy(seq_in_np).to(DTYPE)}
            
            if self.has_globals and "globals" in shard_data:
                glb_np = shard_data["globals"][within_shard_idx]
                if hasattr(glb_np, 'base') and glb_np.base is not None:
                    glb_np = glb_np.copy()
                inputs["global_features"] = torch.from_numpy(glb_np).to(DTYPE)
            
            targets = torch.from_numpy(tgt_np).to(DTYPE)
        
        return inputs, targets


def create_dataset(
    dir_path: Path,
    config: Dict[str, Any],
    indices: List[int],
) -> AtmosphericDataset:
    """
    Create a dataset instance.
    
    Args:
        dir_path: Directory containing processed data
        config: Configuration dictionary
        indices: List of sample indices to use
        
    Returns:
        AtmosphericDataset instance
    """
    logger.info(f"Creating dataset from {dir_path}...")
    return AtmosphericDataset(
        dir_path=dir_path,
        config=config,
        indices=indices,
    )


def pad_collate(
    batch: List[Tuple[Dict[str, Tensor], Tensor]],
    padding_value: float = PADDING_VALUE,
    padding_epsilon: float = 1e-6,
) -> Tuple[Dict[str, Tensor], Dict[str, Tensor], Tensor, Tensor]:
    """
    Collate function with safe padding detection.
    
    Creates padding masks by comparing values to padding sentinel
    within an epsilon tolerance to handle floating point precision.
    
    IMPORTANT MASK CONVENTION:
    - True = padding position (should be ignored)
    - False = valid position (should be processed)
    
    This follows PyTorch's convention for key_padding_mask in attention.
    
    Args:
        batch: List of (inputs, targets) tuples
        padding_value: Sentinel value for padding
        padding_epsilon: Tolerance for padding comparison
        
    Returns:
        Tuple of (batched_inputs, masks, batched_targets, target_masks)
        where masks have True for padding positions
    """
    inputs, targets = zip(*batch)
    
    # Stack sequences
    seq = torch.stack([d["sequence"] for d in inputs])
    
    # Create padding mask (True = padding position)
    # A timestep is considered padding if ALL features equal padding_value
    # This is correct because preprocessing pads all features together
    seq_mask = (torch.abs(seq - padding_value) < padding_epsilon).all(dim=-1)
    
    # Build batched inputs and masks
    batched = {"sequence": seq}
    masks = {"sequence": seq_mask}  # True = padding
    
    # Handle global features if present
    if "global_features" in inputs[0]:
        batched["global_features"] = torch.stack([d["global_features"] for d in inputs])
    
    # Stack targets and create masks
    tgt = torch.stack(targets)
    
    # Target mask: True = padding position
    # All target features at a timestep should be padding_value if it's a padding position
    tgt_mask = (torch.abs(tgt - padding_value) < padding_epsilon).all(dim=-1)
    
    # Validate that sequence and target masks match
    # (padding positions should be the same for inputs and targets)
    if not torch.equal(seq_mask, tgt_mask):
        logger.warning(
            "Sequence and target padding masks don't match! "
            "This indicates a preprocessing issue."
        )
    
    return batched, masks, tgt, tgt_mask


def create_collate_fn(padding_value: float) -> Callable:
    """
    Create a collate function with specified padding value.
    
    Args:
        padding_value: Sentinel value for padding
        
    Returns:
        Partial collate function
    """
    return partial(pad_collate, padding_value=padding_value)


__all__ = ["AtmosphericDataset", "create_dataset", "create_collate_fn"]

===== /Users/imalsky/Desktop/Problemulator/src/utils.py =====
#!/usr/bin/env python3
"""
utils.py - Helper functions for configuration, logging, and data handling.

This module provides utilities for:
- Configuration file loading and validation (with JSON5 support)
- Logging setup with file and console handlers
- Random seed management for reproducibility
- Dataset split generation and loading
- JSON serialization with custom type handlers
- Data integrity checking via hashing
"""
from __future__ import annotations

import hashlib
import json
import logging
import os
import random
from pathlib import Path
from typing import Any, Dict, List, Tuple, Union

import h5py
import numpy as np
import torch

# Try to import JSON5 for comment support in config files
try:
    import json5 as _json_backend
    _HAS_JSON5 = True
except ImportError:
    _json_backend = json
    _HAS_JSON5 = False

# Global constants
DTYPE = torch.float32
PADDING_VALUE = -9999.0
LOG_FORMAT = "%(asctime)s | %(levelname)-8s | %(name)s - %(message)s"
DEFAULT_SEED = 42
METADATA_FILENAME = "normalization_metadata.json"
UTF8_ENCODING = "utf-8"
UTF8_SIG_ENCODING = "utf-8-sig"  # Handle UTF-8 with BOM
HASH_ALGORITHM = "sha256"

logger = logging.getLogger(__name__)


def setup_logging(
    level: int = logging.INFO,
    log_file: Union[str, Path, None] = None,
    force: bool = False,
) -> None:
    """
    Configure logging for the application.
    
    Args:
        level: Logging level (e.g., logging.INFO)
        log_file: Optional file path for logging output
        force: If True, remove existing handlers before setup
    """
    root_logger = logging.getLogger()
    
    # Force reset if requested
    if force:
        while root_logger.handlers:
            handler = root_logger.handlers.pop()
            handler.close()
    
    root_logger.setLevel(level)
    formatter = logging.Formatter(LOG_FORMAT)
    
    # Add console handler if none exists
    if not root_logger.handlers:
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        root_logger.addHandler(console_handler)
    
    # Add file handler if requested
    if log_file:
        try:
            log_file_path = Path(log_file)
            log_file_path.parent.mkdir(parents=True, exist_ok=True)
            file_handler = logging.FileHandler(
                log_file_path, mode="a", encoding=UTF8_ENCODING
            )
            file_handler.setFormatter(formatter)
            root_logger.addHandler(file_handler)
            print(f"Logging to console and file: {log_file_path.resolve()}")
        except OSError as e:
            print(f"Error setting up file logging for {log_file}: {e}. Using console only.")
    else:
        print("Logging to console only.")


def load_config(path: Union[str, Path]) -> Dict[str, Any]:
    """
    Load and validate a configuration file (JSON or JSON5).
    
    Args:
        path: Path to configuration file
        
    Returns:
        Validated configuration dictionary
        
    Raises:
        FileNotFoundError: If config file doesn't exist
        RuntimeError: If config is invalid or malformed
    """
    config_path = Path(path)
    if not config_path.is_file():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    try:
        with open(config_path, "r", encoding=UTF8_SIG_ENCODING) as f:
            if _HAS_JSON5:
                config_dict = _json_backend.load(f)
            else:
                logger.warning("JSON5 not available; comments in config will cause errors.")
                config_dict = json.load(f)
        
        validate_config(config_dict)
        
        backend = "JSON5" if _HAS_JSON5 else "JSON"
        logger.info(f"Loaded and validated {backend} config from {config_path}.")
        return config_dict
        
    except json.JSONDecodeError as e:
        raise RuntimeError(f"Failed to parse JSON from {config_path}: {e}") from e
    except Exception as e:
        raise RuntimeError(f"Failed to load or validate config {config_path}: {e}") from e


def validate_config(config: Dict[str, Any]) -> None:
    """
    Validate configuration structure and required fields.
    
    Args:
        config: Configuration dictionary to validate
        
    Raises:
        ValueError: If configuration is invalid
    """
    # Validate data specification
    data_spec = config.get("data_specification")
    if not isinstance(data_spec, dict):
        raise ValueError("Config section 'data_specification' is missing or not a dictionary.")
    
    if not data_spec.get("input_variables"):
        raise ValueError("'input_variables' must be a non-empty list.")
    
    if not data_spec.get("target_variables"):
        raise ValueError("'target_variables' must be a non-empty list.")
    
    # Validate model hyperparameters
    model_params = config.get("model_hyperparameters")
    if not isinstance(model_params, dict):
        raise ValueError("Config section 'model_hyperparameters' is missing or not a dictionary.")
    
    d_model = model_params.get("d_model", 0)
    nhead = model_params.get("nhead", 0)
    
    if not isinstance(d_model, int) or d_model <= 0:
        raise ValueError("'d_model' must be a positive integer.")
    
    if not isinstance(nhead, int) or nhead <= 0:
        raise ValueError("'nhead' must be a positive integer.")
    
    if d_model % nhead != 0:
        raise ValueError(f"'d_model' ({d_model}) must be divisible by 'nhead' ({nhead}).")


def ensure_dirs(*paths: Union[str, Path, None]) -> bool:
    """
    Create directories if they don't exist.
    
    Args:
        *paths: Variable number of directory paths
        
    Returns:
        True if successful, False otherwise
    """
    try:
        for path in paths:
            if path is not None:
                Path(path).mkdir(parents=True, exist_ok=True)
        return True
    except OSError as e:
        logger.error(f"Failed to create directories {paths}: {e}")
        return False


def _json_serializer(obj: Any) -> Any:
    """
    Custom JSON serializer for NumPy/PyTorch types.
    
    Args:
        obj: Object to serialize
        
    Returns:
        JSON-serializable representation
        
    Raises:
        TypeError: If object type is not supported
    """
    if isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    if isinstance(obj, torch.Tensor):
        return obj.detach().cpu().tolist()
    if isinstance(obj, Path):
        return str(obj.resolve())
    
    raise TypeError(f"Object of type {type(obj).__name__} is not JSON serializable.")


def save_json(data: Dict[str, Any], path: Union[str, Path]) -> bool:
    """
    Save dictionary to JSON file with custom serialization.
    
    Args:
        data: Dictionary to save
        path: Output file path
        
    Returns:
        True if successful, False otherwise
    """
    try:
        json_path = Path(path)
        ensure_dirs(json_path.parent)
        
        with json_path.open("w", encoding=UTF8_ENCODING) as f:
            json.dump(data, f, indent=2, default=_json_serializer, ensure_ascii=False)
            f.flush()
            os.fsync(f.fileno())
        return True
        
    except (OSError, TypeError) as e:
        logger.error(f"Failed to save JSON to {path}: {e}", exc_info=True)
        return False


def seed_everything(seed: int = DEFAULT_SEED) -> None:
    """
    Set random seeds for reproducibility across all libraries.
    
    Args:
        seed: Random seed value
    """
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    logger.info(f"Global random seed set to {seed}.")


def generate_dataset_splits(
    raw_hdf5_paths: List[Path],
    val_frac: float = 0.15,
    test_frac: float = 0.15,
    random_seed: int = DEFAULT_SEED,
) -> Dict[str, List[Tuple[str, int]]]:
    """
    Generate train/validation/test splits from HDF5 files.
    
    Args:
        raw_hdf5_paths: List of HDF5 file paths
        val_frac: Fraction for validation set
        test_frac: Fraction for test set
        random_seed: Seed for reproducible splits
        
    Returns:
        Dictionary with 'train', 'validation', 'test' splits
        
    Raises:
        ValueError: If fractions are invalid
        RuntimeError: If no data found
    """
    # Validate fractions
    if not (0 < val_frac < 1 and 0 < test_frac < 1 and val_frac + test_frac < 1):
        raise ValueError(f"Invalid split fractions: val={val_frac}, test={test_frac}")
    
    all_indices = []
    total_profiles = 0
    
    # Collect all indices from all files
    for h5_path in raw_hdf5_paths:
        if not h5_path.is_file():
            logger.warning(f"Skipping missing HDF5 file: {h5_path}")
            continue
            
        try:
            with h5py.File(h5_path, "r") as hf:
                if not hf.keys():
                    continue
                
                # FIXED: Only check actual datasets with dimensions
                datasets = [k for k in hf.keys() 
                           if isinstance(hf[k], h5py.Dataset) and hf[k].ndim > 0]
                
                if not datasets:
                    continue
                
                n_profiles = hf[datasets[0]].shape[0]
                
                # Verify consistent dimensions across datasets only
                if not all(hf[k].shape[0] == n_profiles for k in datasets):
                    raise AssertionError("Inconsistent leading dimensions in HDF5")
                
                file_stem = h5_path.stem
                all_indices.extend([(file_stem, i) for i in range(n_profiles)])
                total_profiles += n_profiles
                
        except (OSError, AssertionError) as e:
            logger.warning(f"Failed to read {h5_path}: {e}. Skipping.")
    
    if total_profiles == 0:
        raise RuntimeError("No profiles found across all HDF5 files.")
    
    # Calculate split sizes
    n_val = max(1, int(round(total_profiles * val_frac)))
    n_test = max(1, int(round(total_profiles * test_frac)))
    n_train = total_profiles - n_val - n_test
    
    if n_train <= 0:
        raise ValueError(f"Training split empty. Reduce val/test fractions.")
    
    # Shuffle and split
    rng = random.Random(random_seed)
    rng.shuffle(all_indices)
    
    splits = {
        "train": all_indices[:n_train],
        "validation": all_indices[n_train:n_train + n_val],
        "test": all_indices[n_train + n_val:],
    }
    
    logger.info(
        f"Generated splits from {total_profiles} profiles across {len(raw_hdf5_paths)} files: "
        f"train={len(splits['train'])} ({len(splits['train'])/total_profiles:.1%}), "
        f"val={len(splits['validation'])} ({len(splits['validation'])/total_profiles:.1%}), "
        f"test={len(splits['test'])} ({len(splits['test'])/total_profiles:.1%})"
    )
    
    return splits


def get_config_str(config: Dict[str, Any], section: str, key: str, op_desc: str) -> str:
    """
    Safely extract a string value from nested config dictionary.
    
    Args:
        config: Configuration dictionary
        section: Section name in config
        key: Key within section
        op_desc: Operation description for error messages
        
    Returns:
        The string value
        
    Raises:
        ValueError: If section/key missing or invalid
    """
    if section not in config or not isinstance(config[section], dict):
        raise ValueError(f"Config section '{section}' missing or invalid for {op_desc}.")
    
    path_val = config[section].get(key)
    if not isinstance(path_val, str) or not path_val.strip():
        raise ValueError(f"Config key '{key}' in '{section}' missing or empty for {op_desc}.")
    
    return path_val.strip()


def load_or_generate_splits(
    config: Dict[str, Any],
    data_root_dir: Path,
    raw_hdf5_paths: List[Path],
    model_save_dir: Path,
) -> Tuple[Dict[str, List[Tuple[str, int]]], Path]:
    """
    Load existing dataset splits or generate new ones.
    
    Args:
        config: Configuration dictionary
        data_root_dir: Root data directory
        raw_hdf5_paths: List of raw HDF5 files
        model_save_dir: Directory to save generated splits
        
    Returns:
        Tuple of (splits dictionary, splits file path)
    """
    splits_path = None
    
    try:
        # Try to load existing splits
        splits_filename = get_config_str(
            config, "data_paths_config", "dataset_splits_filename", "dataset splits"
        )
        splits_path = data_root_dir / splits_filename
        
        logger.info(f"Loading dataset splits from: {splits_path}")
        
        if not splits_path.exists():
            raise FileNotFoundError(f"Splits file not found: {splits_path}")
        
        with open(splits_path, "r", encoding=UTF8_ENCODING) as f:
            splits = json.load(f)
        
        # Validate splits structure
        required_keys = {"train", "validation", "test"}
        if not required_keys.issubset(splits.keys()):
            raise ValueError(f"Splits file must contain keys: {required_keys}")
        
        for key in required_keys:
            if not isinstance(splits[key], list) or not splits[key]:
                raise ValueError(f"Split '{key}' must be a non-empty list")
        
        # Convert lists to tuples for consistency
        for split in splits.values():
            for i, item in enumerate(split):
                if isinstance(item, list) and len(item) == 2:
                    split[i] = tuple(item)
        
        logger.info(f"Loaded splits from {splits_path}")
        logger.info(
            f"Split sizes: {len(splits['train'])} train, "
            f"{len(splits['validation'])} val, {len(splits['test'])} test."
        )
        return splits, splits_path
        
    except (KeyError, ValueError, FileNotFoundError, json.JSONDecodeError) as e:
        logger.info(f"Could not load splits file. Reason: {e}. Generating new splits.")
    
    # Generate new splits
    logger.info("Generating new dataset splits...")
    
    train_params = config.get("training_hyperparameters", {})
    val_frac = train_params.get("val_frac", 0.15)
    test_frac = train_params.get("test_frac", 0.15)
    
    misc_settings = config.get("miscellaneous_settings", {})
    random_seed = misc_settings.get("random_seed", DEFAULT_SEED)
    
    splits = generate_dataset_splits(
        raw_hdf5_paths=raw_hdf5_paths,
        val_frac=val_frac,
        test_frac=test_frac,
        random_seed=random_seed,
    )
    
    # Save generated splits
    new_splits_path = model_save_dir / "splits_generated.json"
    if save_json(splits, new_splits_path):
        logger.info(f"Saved generated splits to {new_splits_path}")
    else:
        logger.warning("Failed to save generated splits")
    
    return splits, new_splits_path


def compute_data_hash(config: Dict[str, Any], raw_hdf5_paths: List[Path]) -> str:
    """
    Compute hash of configuration and data paths.
    
    Args:
        config: Configuration dictionary
        raw_hdf5_paths: List of HDF5 file paths
        
    Returns:
        Hexadecimal hash string
    """
    hasher = hashlib.new(HASH_ALGORITHM)
    hasher.update(json.dumps(config, sort_keys=True).encode(UTF8_ENCODING))
    
    for path in sorted(raw_hdf5_paths):
        hasher.update(str(path).encode(UTF8_ENCODING))
    
    return hasher.hexdigest()


def compute_data_hash_with_stats(config: Dict[str, Any], raw_hdf5_paths: List[Path]) -> str:
    """
    Compute hash including file modification times and sizes.
    
    More robust than compute_data_hash as it detects in-place file modifications.
    
    Args:
        config: Configuration dictionary
        raw_hdf5_paths: List of HDF5 file paths
        
    Returns:
        Hexadecimal hash string
    """
    hasher = hashlib.new(HASH_ALGORITHM)
    
    # Hash the config
    hasher.update(json.dumps(config, sort_keys=True).encode(UTF8_ENCODING))
    
    # Hash file paths, sizes, and modification times
    for path in sorted(raw_hdf5_paths):
        hasher.update(str(path).encode(UTF8_ENCODING))
        
        if path.is_file():
            stat = path.stat()
            # Include file size and modification time
            hasher.update(str(stat.st_size).encode(UTF8_ENCODING))
            hasher.update(str(stat.st_mtime_ns).encode(UTF8_ENCODING))
        else:
            # Mark missing files explicitly
            hasher.update(b"missing")
    
    return hasher.hexdigest()


__all__ = [
    "DTYPE",
    "PADDING_VALUE",
    "LOG_FORMAT",
    "DEFAULT_SEED",
    "METADATA_FILENAME",
    "setup_logging",
    "load_config",
    "validate_config",
    "ensure_dirs",
    "save_json",
    "seed_everything",
    "generate_dataset_splits",
    "get_config_str",
    "load_or_generate_splits",
    "compute_data_hash",
    "compute_data_hash_with_stats",
]

===== /Users/imalsky/Desktop/Problemulator/src/train.py =====
#!/usr/bin/env python3
"""
train.py - Optimized model training with correct padding mask handling.

Features:
- Async device prefetching for better GPU utilization
- Mixed precision training with AMP
- Gradient accumulation for large effective batch sizes
- Early stopping with patience
- Learning rate scheduling
- Hardware-specific optimizations (A100 detection)
- Proper loss masking for padded sequences

PADDING CONVENTION:
- Mask values: True = padding position, False = valid position
- Loss computation only includes valid positions
- No output overwriting - follows industry standards
"""
from __future__ import annotations

import gc
import logging
import time
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple

import optuna
import torch
from torch import nn, optim
from torch.amp import GradScaler, autocast
from torch.optim.lr_scheduler import (
    CosineAnnealingLR,
    LinearLR,
    ReduceLROnPlateau,
    SequentialLR,
)
from torch.utils.data import DataLoader

from dataset import create_dataset
from hardware import should_pin_memory
from model import create_prediction_model, export_model
from utils import save_json, seed_everything

logger = logging.getLogger(__name__)

# Training defaults
DEFAULT_BATCH_SIZE = 256
DEFAULT_EPOCHS = 100
DEFAULT_LR = 1e-4
DEFAULT_OPTIMIZER = "adamw"
DEFAULT_GRAD_CLIP = 1.0
DEFAULT_EARLY_STOPPING_PATIENCE = 20
DEFAULT_MIN_DELTA = 1e-6
DEFAULT_GRADIENT_ACCUMULATION = 1
DEFAULT_NUM_WORKERS = 8
DEFAULT_MAX_BATCH_FAILURE_RATE = 0.10


class DevicePrefetchLoader:
    """
    Wraps a DataLoader to prefetch batches to device asynchronously.
    
    Overlaps data transfer with computation for better GPU utilization.
    """
    
    def __init__(self, loader: DataLoader, device: torch.device):
        """
        Initialize prefetch loader.
        
        Args:
            loader: Base DataLoader
            device: Target device for data
        """
        self.loader = loader
        self.device = device
        self.is_cuda = self.device.type == 'cuda'
    
    def __iter__(self):
        """Iterate with prefetching for CUDA devices."""
        if self.is_cuda:
            stream = torch.cuda.Stream()
            first = True
            current_batch = None
            
            for next_batch in self.loader:
                # Asynchronously transfer next batch
                with torch.cuda.stream(stream):
                    next_batch = self._to_device(next_batch)
                
                # Yield previous batch while next is transferring
                if not first:
                    yield current_batch
                else:
                    first = False
                
                # Wait for transfer to complete
                torch.cuda.current_stream().wait_stream(stream)
                current_batch = next_batch
            
            # Yield final batch
            if current_batch is not None:
                yield current_batch
        else:
            # No prefetching for non-CUDA devices
            for batch in self.loader:
                yield self._to_device(batch)
    
    def _to_device(self, batch):
        """
        Move batch to target device.
        
        Note: Masks remain as boolean tensors (True = padding).
        """
        inputs, masks, targets, tgt_masks = batch
        non_blocking = self.is_cuda
        
        # Move inputs
        device_inputs = {}
        device_inputs["sequence"] = inputs["sequence"].to(
            self.device, non_blocking=non_blocking
        )
        if "global_features" in inputs:
            device_inputs["global_features"] = inputs["global_features"].to(
                self.device, non_blocking=non_blocking
            )
        
        # Move masks (keep as boolean)
        device_masks = {}
        device_masks["sequence"] = masks["sequence"].to(
            self.device, non_blocking=non_blocking
        )
        
        # Move targets and target masks
        device_targets = targets.to(self.device, non_blocking=non_blocking)
        device_tgt_masks = tgt_masks.to(self.device, non_blocking=non_blocking)
        
        return device_inputs, device_masks, device_targets, device_tgt_masks
    
    def __len__(self):
        """Return number of batches."""
        return len(self.loader)


class ModelTrainer:
    """
    Orchestrates model training, validation, and testing.
    
    Handles all aspects of the training pipeline including:
    - Dataset creation and DataLoader setup
    - Model initialization and optimization
    - Training loop with validation
    - Checkpointing and early stopping
    - Model export for deployment
    - Correct padding mask handling throughout
    """
    
    def __init__(
        self,
        config: Dict[str, Any],
        device: torch.device,
        save_dir: Path,
        processed_dir: Path,
        splits: Dict[str, List[Tuple[str, int]]],
        collate_fn: Callable,
        optuna_trial: Optional[optuna.Trial] = None,
    ) -> None:
        """
        Initialize trainer.
        
        Args:
            config: Configuration dictionary
            device: Compute device
            save_dir: Directory for saving checkpoints
            processed_dir: Directory with preprocessed data
            splits: Train/val/test splits
            collate_fn: Collation function for DataLoader
            optuna_trial: Optional Optuna trial for hyperparameter search
        """
        self.cfg = config
        self.device = device
        self.save_dir = save_dir
        self.model = None
        self.current_epoch = 0
        self.trial = optuna_trial
        
        # Extract configuration
        misc_cfg = self.cfg.get("miscellaneous_settings", {})
        train_params = self.cfg.get("training_hyperparameters", {})
        
        self.max_batch_failure_rate = train_params.get(
            "max_batch_failure_rate", DEFAULT_MAX_BATCH_FAILURE_RATE
        )
        
        # Apply A100-specific optimizations
        if self.device.type == "cuda" and "A100" in torch.cuda.get_device_name(0):
            logger.info("Detected A100 GPU - applying performance optimizations")
            
            if not train_params.get("use_amp", False):
                logger.info("Enabling AMP for A100")
                train_params["use_amp"] = True
            
            current_batch_size = train_params.get("batch_size", DEFAULT_BATCH_SIZE)
            if current_batch_size < 256:
                logger.info(f"Increasing batch size from {current_batch_size} to 256")
                train_params["batch_size"] = 256
        
        # Enable anomaly detection if requested
        if misc_cfg.get("detect_anomaly", False):
            torch.autograd.set_detect_anomaly(True)
            logger.warning("Anomaly detection enabled - training will be slower.")
        
        # Save splits for reproducibility
        save_json(splits, self.save_dir / "dataset_splits.json")
        
        # Initialize components
        self._setup_datasets(processed_dir, splits)
        self._build_dataloaders(collate_fn, misc_cfg, train_params)
        self._build_model()
        self._build_optimizer(train_params)
        self._build_scheduler(train_params)
        self._setup_training_params(train_params)
        self._setup_logging()
        self._save_metadata()
        
        # Check if validation set exists
        self.has_val = self.val_loader is not None and len(self.val_loader) > 0
        
        # Clean up memory
        gc.collect()
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
        
        logger.info("Trainer initialized. Padding convention: True = padding position")
    
    def _setup_datasets(
        self, processed_dir: Path, splits: Dict[str, List[Tuple[str, int]]]
    ) -> None:
        """Create datasets from preprocessed data."""
        # Check if using subset
        fraction = self.cfg.get("training_hyperparameters", {}).get(
            "dataset_fraction_to_use", 1.0
        )
        
        if 0.0 < fraction < 1.0:
            logger.warning(f"Using only {fraction:.0%} of the dataset.")
            random_seed = self.cfg.get("miscellaneous_settings", {}).get(
                "random_seed", 42
            )
            seed_everything(random_seed)
        
        # Dataset directories
        train_dir = processed_dir / "train"
        val_dir = processed_dir / "val"
        test_dir = processed_dir / "test"
        
        if not train_dir.exists():
            raise RuntimeError("No training directory found.")
        
        def get_indices(split_data: List[Tuple[str, int]]) -> List[int]:
            """Get indices, optionally sampling a fraction."""
            num_samples = len(split_data)
            if num_samples == 0:
                return []
            
            if 0.0 < fraction < 1.0:
                import random
                k = max(1, int(num_samples * fraction))
                return random.sample(range(num_samples), k)
            
            return list(range(num_samples))
        
        # Create datasets
        train_idx = get_indices(splits["train"])
        val_idx = get_indices(splits["validation"])
        test_idx = get_indices(splits["test"])
        
        self.train_ds = create_dataset(train_dir, self.cfg, train_idx)
        self.val_ds = (
            create_dataset(val_dir, self.cfg, val_idx) if val_dir.exists() else None
        )
        self.test_ds = (
            create_dataset(test_dir, self.cfg, test_idx) if test_dir.exists() else None
        )
        
        logger.info(
            f"Datasets ready - train:{len(self.train_ds):,}  "
            f"val:{len(self.val_ds or []):,}  test:{len(self.test_ds or []):,}"
        )
    
    def _build_dataloaders(
        self, collate_fn: Callable, misc_cfg: Dict, train_cfg: Dict
    ) -> None:
        """Create DataLoaders with optimized settings."""
        pin_memory = should_pin_memory()
        num_workers = misc_cfg.get("num_workers", DEFAULT_NUM_WORKERS)
        
        # Ensure enough workers for GPU
        if num_workers < 8 and self.device.type == "cuda":
            logger.info(f"Increasing num_workers from {num_workers} to 8 for better GPU utilization")
            num_workers = 8
        
        # Common DataLoader arguments
        dl_common = dict(
            batch_size=train_cfg.get("batch_size", DEFAULT_BATCH_SIZE),
            num_workers=num_workers,
            collate_fn=collate_fn,
            pin_memory=pin_memory,
        )
        
        # Add persistent workers if using multiple workers
        if num_workers > 0:
            dl_common["persistent_workers"] = True
            dl_common["prefetch_factor"] = 4
        
        # Create DataLoaders
        self.train_loader = DataLoader(
            self.train_ds, shuffle=True, drop_last=False, **dl_common
        )
        
        self.val_loader = (
            DataLoader(self.val_ds, shuffle=False, drop_last=False, **dl_common)
            if self.val_ds
            else None
        )
        
        self.test_loader = (
            DataLoader(self.test_ds, shuffle=False, drop_last=False, **dl_common)
            if self.test_ds
            else None
        )
        
        # Wrap with device prefetching for async transfers
        if self.device.type != "cpu":
            logger.info(f"Using DevicePrefetchLoader for {self.device.type.upper()} transfers")
            
            self.train_loader = DevicePrefetchLoader(self.train_loader, self.device)
            if self.val_loader:
                self.val_loader = DevicePrefetchLoader(self.val_loader, self.device)
            if self.test_loader:
                self.test_loader = DevicePrefetchLoader(self.test_loader, self.device)
    
    def _build_model(self) -> None:
        """Create and initialize the model."""
        self.model = create_prediction_model(
            self.cfg, device=self.device, compile_model=True
        )
    
    def _build_optimizer(self, tp: Dict) -> None:
        """Create optimizer with weight decay groups."""
        opt_name = tp.get("optimizer", DEFAULT_OPTIMIZER).lower()
        lr = tp.get("learning_rate", DEFAULT_LR)
        wd = tp.get("weight_decay", 1e-5)
        
        # Separate parameters for weight decay
        decay, no_decay = [], []
        for n, p in self.model.named_parameters():
            if not p.requires_grad:
                continue
            
            # Don't apply weight decay to biases and normalization
            if p.dim() == 1 or "bias" in n or "norm" in n:
                no_decay.append(p)
            else:
                decay.append(p)
        
        # Parameter groups with different weight decay
        groups = [
            {"params": decay, "weight_decay": wd},
            {"params": no_decay, "weight_decay": 0.0},
        ]
        
        # Create optimizer
        if opt_name == "adam":
            self.optimizer = optim.Adam(groups, lr=lr)
        elif opt_name == "sgd":
            self.optimizer = optim.SGD(groups, lr=lr, momentum=0.9)
        else:  # Default to AdamW
            self.optimizer = optim.AdamW(groups, lr=lr)
        
        logger.info(f"Optimizer: {opt_name}  lr={lr:.2e}  wd={wd:.2e}")
    
    def _build_scheduler(self, tp: Dict) -> None:
        """Create learning rate scheduler."""
        scheduler_type = tp.get("scheduler_type", "reduce_on_plateau").lower()
        
        if scheduler_type == "cosine":
            epochs = tp.get("epochs", DEFAULT_EPOCHS)
            warmup_epochs = tp.get("warmup_epochs", 0)
            
            if warmup_epochs >= epochs:
                raise ValueError("warmup_epochs must be less than total epochs.")
            
            # Main cosine scheduler
            main_scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=epochs - warmup_epochs,
                eta_min=tp.get("min_lr", 1e-8),
            )
            
            if warmup_epochs > 0:
                # Add warmup scheduler
                warmup_scheduler = LinearLR(
                    self.optimizer,
                    start_factor=1e-5,
                    end_factor=1.0,
                    total_iters=warmup_epochs,
                )
                
                self.scheduler = SequentialLR(
                    self.optimizer,
                    schedulers=[warmup_scheduler, main_scheduler],
                    milestones=[warmup_epochs],
                )
                logger.info(f"Cosine scheduler with {warmup_epochs} warmup epochs.")
            else:
                self.scheduler = main_scheduler
                logger.info("Cosine scheduler without warmup.")
        
        elif scheduler_type == "reduce_on_plateau":
            self.scheduler = ReduceLROnPlateau(
                self.optimizer,
                mode="min",
                patience=tp.get("lr_patience", 10),
                factor=tp.get("lr_factor", 0.5),
                min_lr=tp.get("min_lr", 1e-8),
            )
            logger.info("Using ReduceLROnPlateau scheduler.")
        
        else:
            raise ValueError(f"Unsupported scheduler_type: {scheduler_type}")
    
    def _setup_training_params(self, tp: Dict) -> None:
        """Setup training parameters and loss function."""
        # Loss function (MSE with no reduction for masking)
        self.criterion = nn.MSELoss(reduction="none")
        
        # Gradient clipping
        self.max_grad_norm = tp.get("gradient_clip_val", DEFAULT_GRAD_CLIP)
        
        # Gradient accumulation
        self.accumulation_steps = tp.get(
            "gradient_accumulation_steps", DEFAULT_GRADIENT_ACCUMULATION
        )
        
        # Mixed precision training
        self.use_amp = tp.get("use_amp", False) and self.device.type == "cuda"
        self.scaler = GradScaler(enabled=self.use_amp)
        
        if self.use_amp:
            logger.info("AMP (Automatic Mixed Precision) enabled.")
        
        # Log effective batch size
        eff_bs = tp.get("batch_size", DEFAULT_BATCH_SIZE) * self.accumulation_steps
        logger.info(
            f"Effective batch size: {eff_bs} "
            f"(batch_size={tp.get('batch_size', DEFAULT_BATCH_SIZE)} × "
            f"accumulation={self.accumulation_steps})"
        )
    
    def _setup_logging(self) -> None:
        """Setup training log file."""
        self.log_path = self.save_dir / "training_log.csv"
        self.log_path.write_text("epoch,train_loss,val_loss,lr,time_s,improvement\n")
        
        self.best_val_loss = float("inf")
        self.best_epoch = -1
    
    def _save_metadata(self) -> None:
        """Save training metadata."""
        metadata = {
            "device": str(self.device),
            "use_amp": self.use_amp,
            "effective_batch_size": (
                self.cfg["training_hyperparameters"].get("batch_size", DEFAULT_BATCH_SIZE)
                * self.accumulation_steps
            ),
            "num_parameters": sum(p.numel() for p in self.model.parameters()),
            "num_trainable": sum(p.numel() for p in self.model.parameters() if p.requires_grad),
            "padding_convention": "True = padding position (PyTorch standard)",
        }
        save_json(metadata, self.save_dir / "training_metadata.json")
    
    def train(self) -> float:
        """
        Run the training loop.
        
        Returns:
            Best validation loss achieved
        """
        tp = self.cfg.get("training_hyperparameters", {})
        epochs = tp.get("epochs", DEFAULT_EPOCHS)
        patience = tp.get("early_stopping_patience", DEFAULT_EARLY_STOPPING_PATIENCE)
        min_delta = tp.get("min_delta", DEFAULT_MIN_DELTA)
        
        epochs_without_improvement = 0
        
        logger.info(f"Starting training for {epochs} epochs.")
        
        for epoch in range(1, epochs + 1):
            self.current_epoch = epoch
            epoch_start = time.time()
            
            # Training epoch
            train_loss = self._run_epoch(self.train_loader, is_train=True)
            if train_loss is None:
                raise RuntimeError("Too many invalid batches in training.")
            
            # Validation epoch
            val_loss = (
                self._run_epoch(self.val_loader, is_train=False)
                if self.has_val
                else float("inf")
            )
            if val_loss is None:
                val_loss = float("inf")
            
            # Update learning rate
            if isinstance(self.scheduler, ReduceLROnPlateau):
                metric = val_loss if self.has_val and val_loss != float("inf") else train_loss
                self.scheduler.step(metric)
            else:
                self.scheduler.step()
            
            # Report to Optuna if in hyperparameter search
            if self.trial:
                self.trial.report(val_loss, epoch)
                if self.trial.should_prune():
                    logger.info("Trial pruned by Optuna.")
                    raise optuna.exceptions.TrialPruned()
            
            # Calculate improvement and log results
            improvement = self.best_val_loss - val_loss if self.has_val else 0.0
            self._log_epoch_results(
                epoch, train_loss, val_loss, time.time() - epoch_start, improvement
            )
            
            # Check for improvement and early stopping
            if self.has_val:
                if val_loss < self.best_val_loss - min_delta:
                    # Improvement found
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    epochs_without_improvement = 0
                    self._save_best_model()
                else:
                    # No improvement
                    epochs_without_improvement += 1
                    
                    if epochs_without_improvement >= patience:
                        logger.info(
                            f"Early stopping triggered after {patience} epochs "
                            f"without improvement."
                        )
                        break
        
        # If no validation set, save final model
        if not self.has_val:
            self.best_val_loss = train_loss
            self.best_epoch = epochs
            self._save_best_model()
        
        logger.info(
            f"Training complete. Best val_loss={self.best_val_loss:.4e} "
            f"at epoch {self.best_epoch}."
        )
        
        return self.best_val_loss
    
    def test(self) -> Dict[str, float]:
        """
        Run testing on the test set.
        
        Returns:
            Dictionary with test metrics
        """
        if not self.test_loader:
            logger.warning("No test dataset available. Skipping test.")
            return {"test_loss": float("inf"), "best_epoch": self.best_epoch}
        
        # Load best model checkpoint
        checkpoint_path = self.save_dir / "best_model.pt"
        if checkpoint_path.exists():
            state = torch.load(checkpoint_path, map_location=self.device)
            state_dict = state["state_dict"]
            
            # Handle compiled model state dict
            if any(k.startswith("_orig_mod.") for k in state_dict):
                state_dict = {
                    k.replace("_orig_mod.", ""): v
                    for k, v in state_dict.items()
                }
            
            self.model.load_state_dict(state_dict, strict=True)
            logger.info(f"Loaded best model from epoch {state['epoch']}")
        
        # Run test evaluation
        test_loss = self._run_epoch(self.test_loader, is_train=False)
        if test_loss is None:
            test_loss = float("inf")
        
        # Save test metrics
        metrics = {
            "test_loss": test_loss,
            "best_epoch": self.best_epoch,
        }
        save_json(metrics, self.save_dir / "test_metrics.json")
        
        logger.info(f"Test loss: {test_loss:.4e}")
        
        return metrics
    
    def _run_epoch(
        self, loader: Optional[DataLoader], is_train: bool
    ) -> Optional[float]:
        """
        Run one epoch of training or validation.
        
        IMPORTANT: This method correctly handles padding masks:
        - target_masks has True for padding positions
        - valid_mask = ~target_masks (True for valid positions)
        - Loss is computed only on valid positions
        
        Args:
            loader: DataLoader for the epoch
            is_train: Whether this is a training epoch
            
        Returns:
            Average loss for the epoch, or None if too many batches failed
        """
        if not loader or len(loader) == 0:
            mode = "training" if is_train else "validation"
            logger.warning(f"DataLoader for {mode} is empty. Skipping epoch.")
            return 0.0 if is_train else float("inf")
        
        # Set model mode
        self.model.train(is_train)
        
        # Track statistics
        total_loss = 0.0
        total_elements = 0
        failed_batches = 0
        
        device_type = str(self.device.type)
        
        for batch_idx, batch in enumerate(loader):
            # Data already on device if using DevicePrefetchLoader
            inputs, masks, targets, target_masks = batch
            
            sequence = inputs["sequence"]
            global_features = inputs.get("global_features")
            sequence_mask = masks["sequence"]  # True = padding position
            
            # Forward pass with autocast
            with torch.set_grad_enabled(is_train), autocast(
                device_type=device_type, enabled=self.use_amp
            ):
                # Model forward
                predictions = self.model(sequence, global_features, sequence_mask)
                
                # Compute unreduced loss
                unreduced_loss = self.criterion(predictions, targets)
                
                # IMPORTANT: Create valid mask (inverse of padding mask)
                # target_masks has True for padding, we want True for valid
                valid_mask = (~target_masks).unsqueeze(-1).expand_as(unreduced_loss)
                
                # Count valid elements
                num_valid = valid_mask.sum()
                
                # Skip batch if all elements are padding
                if num_valid.item() == 0:
                    failed_batches += 1
                    logger.debug(f"Batch {batch_idx} has no valid elements (all padding)")
                    continue
                
                # Compute masked loss
                # Only valid positions contribute to the loss
                masked_loss = unreduced_loss * valid_mask.float()
                loss = masked_loss.sum() / num_valid.float()
            
            if is_train:
                # Backward pass
                if self.use_amp:
                    self.scaler.scale(loss).backward()
                else:
                    loss.backward()
                
                # Gradient accumulation and optimization step
                if (batch_idx + 1) % self.accumulation_steps == 0 or (batch_idx + 1) == len(loader):
                    # Unscale gradients if using AMP
                    if self.use_amp:
                        self.scaler.unscale_(self.optimizer)
                    
                    # Gradient clipping
                    if self.max_grad_norm > 0:
                        torch.nn.utils.clip_grad_norm_(
                            self.model.parameters(), self.max_grad_norm
                        )
                    
                    # Optimizer step
                    if self.use_amp:
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        self.optimizer.step()
                    
                    # Zero gradients
                    self.optimizer.zero_grad(set_to_none=True)
            
            # Accumulate statistics
            total_loss += loss.item() * num_valid.item()
            total_elements += num_valid.item()
        
        # Check failure rate
        if len(loader) > 0 and failed_batches / len(loader) > self.max_batch_failure_rate:
            logger.critical(
                f"Too many failed batches ({failed_batches}/{len(loader)}). "
                f"Aborting epoch."
            )
            return None
        
        if total_elements == 0:
            logger.error("No valid elements were processed in this epoch.")
            return None
        
        # Log padding statistics
        if failed_batches > 0:
            logger.debug(
                f"Epoch complete. {failed_batches} batches were all padding. "
                f"Processed {total_elements} valid elements."
            )
        
        # Return average loss
        return total_loss / total_elements
    
    def _log_epoch_results(
        self,
        epoch: int,
        train_loss: float,
        val_loss: float,
        elapsed_time: float,
        improvement: float,
    ) -> None:
        """Log epoch results to console and file."""
        lr = self.optimizer.param_groups[0]["lr"]
        
        # Console log
        msg = (
            f"Epoch {epoch:03d}  "
            f"train:{train_loss:.3e}  "
            f"val:{val_loss:.3e}  "
            f"lr:{lr:.2e}  "
            f"time:{elapsed_time:.1f}s"
        )
        
        if improvement > 0:
            msg += f"  ↓{improvement:.3e}"
        
        logger.info(msg)
        
        # File log
        with self.log_path.open("a") as f:
            f.write(
                f"{epoch},{train_loss:.6e},{val_loss:.6e},"
                f"{lr:.6e},{elapsed_time:.1f},{improvement:.6e}\n"
            )
    
    def _save_best_model(self) -> None:
        """Save the best model checkpoint and export it."""
        # Prepare checkpoint
        checkpoint = {
            "state_dict": self.model.state_dict(),
            "epoch": self.best_epoch,
            "val_loss": self.best_val_loss,
            "config": self.cfg,
            "padding_info": {
                "convention": "True = padding position",
                "follows_pytorch_standard": True,
                "loss_excludes_padding": True,
                "model_output_not_masked": True,
            }
        }
        
        # Save checkpoint
        checkpoint_path = self.save_dir / "best_model.pt"
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"Saved best model (epoch {self.best_epoch}).")
        
        # Export model for deployment
        self._export_model()
    
    def _export_model(self) -> None:
        """Export model using torch.export."""
        try:
            # Get sample input from validation loader
            sample = next(iter(self.val_loader)) if self.val_loader else None
            if sample is None:
                logger.warning("No sample available for export.")
                return
            
            # Create fresh un-compiled model for export
            logger.info("Creating a fresh, un-compiled model instance for export.")
            model_for_export = create_prediction_model(
                self.cfg, device=self.device, compile_model=False
            )
            
            # Load trained weights
            if hasattr(self.model, '_orig_mod'):
                state_dict = self.model._orig_mod.state_dict()
            else:
                state_dict = self.model.state_dict()
            
            model_for_export.load_state_dict(state_dict)
            
            # Prepare example input
            inputs, masks, _, _ = sample
            example = {
                "sequence": inputs["sequence"][:1],
                "sequence_mask": masks["sequence"][:1],  # True = padding
            }
            if "global_features" in inputs:
                example["global_features"] = inputs["global_features"][:1]
            
            # Export model
            export_path = self.save_dir / f"best_model_epoch_{self.best_epoch}"
            export_model(model_for_export, example, export_path, self.cfg)
            
        except Exception as e:
            logger.error(f"Model export failed: {e}", exc_info=True)


__all__ = ["ModelTrainer", "DevicePrefetchLoader"]

===== /Users/imalsky/Desktop/Problemulator/src/hardware.py =====
#!/usr/bin/env python3
"""
hardware.py - Device detection and DataLoader optimization helpers.

This module provides utilities for:
- Automatic device selection (CUDA → MPS → CPU)
- Memory pinning optimization for DataLoaders
- Hardware-specific configuration
"""
from __future__ import annotations

import logging
import torch

logger = logging.getLogger(__name__)


def setup_device() -> torch.device:
    """
    Select the best available compute device.
    
    Priority order:
    1. CUDA (NVIDIA GPUs)
    2. MPS (Apple Silicon)
    3. CPU (fallback)
    
    Returns:
        torch.device: The selected compute device
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
        try:
            device_name = torch.cuda.get_device_name(torch.cuda.current_device())
            logger.info(f"Using CUDA device: {device_name}")
        except Exception:
            logger.info("Using CUDA device.")
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = torch.device("mps")
        logger.info("Using Apple Silicon MPS device (may be slower than CUDA).")
    else:
        device = torch.device("cpu")
        logger.info("Using CPU device.")
    
    return device


def should_pin_memory() -> bool:
    """
    Determine if memory pinning should be enabled for DataLoaders.
    
    Memory pinning speeds up CPU→GPU transfers but uses additional RAM.
    Only beneficial when CUDA is available.
    
    Returns:
        bool: True if memory should be pinned, False otherwise
    """
    return torch.cuda.is_available()


__all__ = ["setup_device", "should_pin_memory"]

===== /Users/imalsky/Desktop/Problemulator/src/normalizer.py =====
#!/usr/bin/env python3
"""
normalizer.py - GPU-aware data normalization with numerical stability.

Calculates global statistics from training samples across multiple HDF5 files
using memory-efficient batch processing with online algorithms.
All logarithmic operations are performed in base 10.
"""
from __future__ import annotations

import logging
import math
import time
from pathlib import Path
from typing import Any, Dict, List, Set, Tuple, Union

import h5py
import numpy as np
import torch
from torch import Tensor
from tqdm import tqdm

from hardware import setup_device
from utils import DTYPE

logger = logging.getLogger(__name__)

# Normalization constants
DEFAULT_EPSILON = 1e-9
DEFAULT_QUANTILE_MEMORY_LIMIT = 1_000_000
DEFAULT_SYMLOG_PERCENTILE = 0.5
STATS_CHUNK_SIZE = 8192
NORMALIZED_VALUE_CLAMP = 50.0


class DataNormalizer:
    """
    Handles data normalization with multiple methods and numerical stability.
    
    Supports methods:
    - standard: Zero mean, unit variance
    - log-standard: Log transform then standardize
    - iqr: Interquartile range normalization
    - log-min-max: Log transform then min-max scaling
    - max-out: Scale by maximum absolute value
    - signed-log: Signed log transform
    - scaled_signed_offset_log: Scaled signed log with offset
    - symlog: Symmetric log (linear near zero, log elsewhere)
    - bool: Binary values (no normalization)
    - none: No normalization
    """
    
    METHODS = {
        "iqr",
        "log-min-max",
        "max-out",
        "signed-log",
        "scaled_signed_offset_log",
        "symlog",
        "standard",
        "log-standard",
        "bool",
        "none",
    }
    
    QUANTILE_METHODS = {"iqr", "symlog", "log-min-max"}
    
    def __init__(self, *, config_data: Dict[str, Any]):
        """
        Initialize normalizer with configuration.
        
        Args:
            config_data: Full configuration dictionary
        """
        self.config = config_data
        self.device = setup_device()
        self.norm_config = self.config.get("normalization", {})
        self.eps = float(self.norm_config.get("epsilon", DEFAULT_EPSILON))
        self.keys_to_process, self.key_methods = self._get_keys_and_methods()
        self._approximated_quantile_keys = set()
        
        logger.info(f"DataNormalizer initialized on device '{self.device}'.")
    
    def _get_keys_and_methods(self) -> Tuple[Set[str], Dict[str, str]]:
        """Extract variables and their normalization methods from config."""
        spec = self.config.get("data_specification", {})
        all_vars = set()
        
        # Collect all variables
        for key in ["input_variables", "global_variables", "target_variables"]:
            all_vars.update(spec.get(key, []))
        
        # Assign normalization methods
        user_key_methods = self.norm_config.get("key_methods", {})
        default_method = self.norm_config.get("default_method", "standard")
        
        key_methods = {}
        for key in all_vars:
            method = user_key_methods.get(key, default_method).lower()
            if method not in self.METHODS:
                raise ValueError(f"Unsupported method '{method}' for key '{key}'.")
            key_methods[key] = method
        
        return all_vars, key_methods
    
    def calculate_stats(
        self, raw_hdf5_paths: List[Path], train_indices: List[Tuple[str, int]]
    ) -> Dict[str, Any]:
        """
        Calculate normalization statistics from training data.
        
        Args:
            raw_hdf5_paths: List of HDF5 file paths
            train_indices: List of (file_stem, index) tuples for training set
            
        Returns:
            Dictionary containing normalization metadata and statistics
        """
        logger.info(
            f"Starting statistics calculation from {len(train_indices)} training samples..."
        )
        start_time = time.time()
        
        if not train_indices:
            raise ValueError("Cannot calculate statistics from empty training indices.")
        
        # Create file mapping
        file_map = {path.stem: path for path in raw_hdf5_paths if path.is_file()}
        if not file_map:
            raise RuntimeError("No valid HDF5 files found.")
        
        # Get available keys and validate
        available_keys = self._get_available_keys(file_map)
        keys_to_load = self.keys_to_process.intersection(available_keys)
        
        if len(keys_to_load) != len(self.keys_to_process):
            missing = self.keys_to_process - available_keys
            logger.warning(f"Keys not found in HDF5 and will be skipped: {missing}")
        
        # Initialize accumulators for online statistics
        accumulators = self._initialize_accumulators(keys_to_load)
        
        # Dataset metadata tracking
        dataset_metadata = {
            "total_train_profiles": len(train_indices),
            "variables": list(keys_to_load),
            "sequence_lengths": {
                key: {"min": float("inf"), "max": float("-inf")} for key in keys_to_load
            },
        }
        
        # Process data in chunks
        grouped_indices = self._group_indices_by_file(train_indices)
        
        for file_stem, indices in grouped_indices.items():
            if file_stem not in file_map:
                logger.warning(f"Skipping unknown file stem '{file_stem}' in indices.")
                continue
            
            h5_path = file_map[file_stem]
            with h5py.File(h5_path, "r", swmr=True, libver="latest") as hf:
                num_chunks = (len(indices) + STATS_CHUNK_SIZE - 1) // STATS_CHUNK_SIZE
                
                for i in tqdm(
                    range(0, len(indices), STATS_CHUNK_SIZE),
                    desc=f"Stats for {file_stem}",
                    total=num_chunks,
                    leave=False,
                ):
                    chunk_indices = indices[i:i + STATS_CHUNK_SIZE]
                    
                    # Sort indices for efficient HDF5 reading
                    chunk_indices_np = np.array(chunk_indices)
                    sorter = np.argsort(chunk_indices_np)
                    sorted_indices = chunk_indices_np[sorter].tolist()
                    
                    # Load batch of data
                    batch_of_tensors = {}
                    for key in keys_to_load:
                        if key in hf:
                            data_chunk_np = hf[key][sorted_indices]
                            batch_of_tensors[key] = torch.from_numpy(data_chunk_np).to(
                                device=self.device, dtype=DTYPE
                            )
                    
                    # Update statistics
                    self._update_accumulators_with_batch(
                        batch_of_tensors, accumulators, dataset_metadata
                    )
        
        logger.info(f"Finished statistics calculation in {time.time() - start_time:.2f}s.")
        
        # Finalize statistics
        computed_stats = self._finalize_stats(accumulators)
        if not computed_stats:
            raise RuntimeError("No statistics computed due to invalid data.")
        
        # Prepare metadata
        metadata = {
            "normalization_methods": self.key_methods,
            "per_key_stats": computed_stats,
        }
        
        # Clean up sequence length metadata
        for key in dataset_metadata["sequence_lengths"]:
            min_len = dataset_metadata["sequence_lengths"][key]["min"]
            max_len = dataset_metadata["sequence_lengths"][key]["max"]
            if min_len == float("inf"):
                dataset_metadata["sequence_lengths"][key] = None
            else:
                dataset_metadata["sequence_lengths"][key] = {
                    "min": int(min_len),
                    "max": int(max_len),
                }
        
        metadata["dataset_metadata"] = dataset_metadata
        
        logger.info("Statistics calculation complete.")
        return metadata
    
    def _get_available_keys(self, file_map: Dict[str, Path]) -> Set[str]:
        """Get all available keys across HDF5 files."""
        available = set()
        for path in file_map.values():
            with h5py.File(path, "r") as hf:
                available.update(hf.keys())
        return available
    
    def _group_indices_by_file(
        self, indices: List[Tuple[str, int]]
    ) -> Dict[str, List[int]]:
        """Group indices by file for efficient loading."""
        grouped = {}
        for file_stem, idx in indices:
            grouped.setdefault(file_stem, []).append(idx)
        return grouped
    
    def _initialize_accumulators(
        self, keys_to_process: Set[str]
    ) -> Dict[str, Dict[str, Any]]:
        """Initialize statistics accumulators for each variable."""
        accumulators = {}
        
        for key in keys_to_process:
            method = self.key_methods[key]
            if method in ("none", "bool"):
                continue
            
            acc: Dict[str, Any] = {}
            
            # Online mean/variance for standard methods
            if method in ("standard", "log-standard", "signed-log"):
                acc.update({
                    "count": 0,
                    "mean": torch.tensor(0.0, dtype=DTYPE, device=self.device),
                    "m2": torch.tensor(0.0, dtype=DTYPE, device=self.device),
                })
            
            # Value collection for quantile methods
            if method in self.QUANTILE_METHODS:
                acc["values"] = torch.empty(0, dtype=DTYPE, device=self.device)
                acc["total_values_seen"] = 0
            
            # Min/max tracking
            if method in ("max-out", "scaled_signed_offset_log", "log-min-max"):
                acc.update({
                    "min": torch.tensor(float("inf"), dtype=DTYPE, device=self.device),
                    "max": torch.tensor(float("-inf"), dtype=DTYPE, device=self.device),
                })
            
            if acc:
                accumulators[key] = acc
        
        return accumulators
    
    def _update_accumulators_with_batch(
        self, batch: Dict[str, Tensor], accumulators: Dict, dataset_metadata: Dict
    ) -> None:
        """Update accumulators with a batch of data using online algorithms."""
        memory_limit = self.norm_config.get(
            "quantile_max_values_in_memory", DEFAULT_QUANTILE_MEMORY_LIMIT
        )
        
        for key, data_batch in batch.items():
            if key not in accumulators:
                continue
            
            method = self.key_methods[key]
            key_acc = accumulators[key]
            
            # Flatten and filter valid data
            data = data_batch.flatten()
            valid_data = data[torch.isfinite(data)]
            
            if valid_data.numel() == 0:
                continue
            
            # Transform data for statistics if needed
            data_for_stats = valid_data
            
            if method == "log-standard":
                valid_data = torch.clamp(valid_data, min=self.eps)
                data_for_stats = torch.log10(valid_data)
            elif method == "signed-log":
                data_for_stats = torch.sign(valid_data) * torch.log10(
                    torch.abs(valid_data) + 1.0
                )
            
            # Update online mean/variance using Welford's algorithm
            if "count" in key_acc:
                n_new = data_for_stats.numel()
                if n_new > 0:
                    count_old = key_acc["count"]
                    mean_old = key_acc["mean"]
                    m2_old = key_acc["m2"]
                    
                    batch_mean = data_for_stats.mean()
                    batch_var = torch.var(data_for_stats, unbiased=False)
                    batch_m2 = batch_var * n_new
                    
                    delta = batch_mean - mean_old
                    count_new = count_old + n_new
                    mean_new = mean_old + delta * (n_new / count_new)
                    m2_new = (
                        m2_old + batch_m2 + delta**2 * count_old * n_new / count_new
                    )
                    
                    key_acc["count"] = count_new
                    key_acc["mean"] = mean_new
                    key_acc["m2"] = m2_new
            
            # Collect values for quantile computation
            if "values" in key_acc:
                key_acc["total_values_seen"] += valid_data.numel()
                current_stored_size = key_acc["values"].numel()
                
                if current_stored_size + valid_data.numel() <= memory_limit:
                    # Still within memory limit
                    key_acc["values"] = torch.cat([key_acc["values"], valid_data])
                else:
                    # Use reservoir sampling for approximation
                    self._approximated_quantile_keys.add(key)
                    combined_data = torch.cat([key_acc["values"], valid_data])
                    perm = torch.randperm(combined_data.numel(), device=self.device)[
                        :memory_limit
                    ]
                    key_acc["values"] = combined_data[perm]
            
            # Update min/max
            if "max" in key_acc:
                if method == "scaled_signed_offset_log":
                    log_vals = torch.sign(valid_data) * torch.log10(
                        torch.abs(valid_data) + 1.0
                    )
                    key_acc["min"] = torch.min(key_acc["min"], log_vals.min())
                    key_acc["max"] = torch.max(key_acc["max"], log_vals.max())
                elif method == "log-min-max":
                    log_vals = torch.log10(torch.clamp(valid_data, min=self.eps))
                    key_acc["min"] = torch.min(key_acc["min"], log_vals.min())
                    key_acc["max"] = torch.max(key_acc["max"], log_vals.max())
                else:
                    key_acc["min"] = torch.min(key_acc["min"], valid_data.min())
                    key_acc["max"] = torch.max(key_acc["max"], valid_data.max())
            
            # Track sequence lengths
            if data_batch.ndim == 2:
                current_len = data_batch.shape[1]
                dataset_metadata["sequence_lengths"][key]["min"] = min(
                    dataset_metadata["sequence_lengths"][key]["min"], current_len
                )
                dataset_metadata["sequence_lengths"][key]["max"] = max(
                    dataset_metadata["sequence_lengths"][key]["max"], current_len
                )
    
    def _finalize_stats(self, accumulators: Dict) -> Dict[str, Any]:
        """Finalize statistics from accumulators with zero-variance protection."""
        final_stats = {}
        
        for key, method in self.key_methods.items():
            stats: Dict[str, Any] = {"method": method, "epsilon": self.eps}
            
            if key not in accumulators:
                if method not in ("none", "bool"):
                    stats["method"] = "none"
                final_stats[key] = stats
                continue
            
            key_acc = accumulators[key]
            
            # Finalize mean/variance statistics with zero-variance protection
            if "count" in key_acc and key_acc["count"] > 1:
                mean = key_acc["mean"].item()
                variance = key_acc["m2"].item() / (key_acc["count"] - 1)
                # CRITICAL: Ensure std is never zero
                std = max(math.sqrt(variance), self.eps)
                
                if method == "standard":
                    stats.update({"mean": mean, "std": std})
                elif method == "log-standard":
                    stats.update({"log_mean": mean, "log_std": std})
                elif method == "signed-log":
                    stats.update({"mean": mean, "std": std})
            elif "count" in key_acc:
                # Single value case - use epsilon for std
                mean = key_acc["mean"].item() if key_acc["count"] > 0 else 0.0
                if method == "standard":
                    stats.update({"mean": mean, "std": self.eps})
                elif method == "log-standard":
                    stats.update({"log_mean": mean, "log_std": self.eps})
                elif method == "signed-log":
                    stats.update({"mean": mean, "std": self.eps})
            
            # Compute quantile statistics
            if "values" in key_acc and key_acc["values"].numel() > 0:
                if key in self._approximated_quantile_keys:
                    logger.info(
                        f"Approximating quantiles for '{key}' using sample of "
                        f"{key_acc['values'].numel():,} values "
                        f"(out of {key_acc['total_values_seen']:,} total)."
                    )
                
                all_values = key_acc["values"]
                stats.update(self._compute_quantile_stats(all_values, key, method))
            
            # Finalize max-out statistics
            if method == "max-out":
                max_val = max(
                    abs(key_acc["min"].item()),
                    abs(key_acc["max"].item())
                )
                # Ensure max_val is never zero
                stats["max_val"] = max(max_val, self.eps)
            
            # Finalize scaled signed offset log
            elif method == "scaled_signed_offset_log":
                m = max(
                    abs(key_acc["min"].item()),
                    abs(key_acc["max"].item()),
                    self.eps
                )
                stats["m"] = m
            
            # Finalize log-min-max with range protection
            elif method == "log-min-max" and "min" in key_acc:
                min_val = key_acc["min"].item()
                max_val = key_acc["max"].item()
                # Ensure non-zero range
                if max_val - min_val < self.eps:
                    max_val = min_val + self.eps
                stats.update({"min": min_val, "max": max_val})
            
            final_stats[key] = stats
        
        return final_stats
    
    def _compute_quantile_stats(
        self, values: Tensor, key: str, method: str
    ) -> dict:
        """Compute quantile-based statistics with improved numerical stability."""
        stats: Dict[str, float] = {}
        
        def _robust_quantile(tensor: Tensor, q_values: Union[float, Tensor]) -> Tensor:
            """Compute quantiles with fallback for memory issues."""
            try:
                return torch.quantile(tensor, q_values)
            except RuntimeError as e:
                if "too large" in str(e).lower() or "out of memory" in str(e).lower():
                    fallback_size = 1_000_000
                    if tensor.numel() <= fallback_size:
                        raise e
                    
                    logger.warning(
                        f"Quantile failed for '{key}' on {tensor.numel():,} elements. "
                        f"Subsampling to {fallback_size:,}."
                    )
                    
                    perm = torch.randperm(tensor.numel(), device=tensor.device)[
                        :fallback_size
                    ]
                    subsampled = tensor.flatten()[perm]
                    return torch.quantile(subsampled, q_values)
                raise e
        
        if method == "iqr":
            q_tensor = torch.tensor([0.25, 0.5, 0.75], dtype=DTYPE, device=values.device)
            q_vals = _robust_quantile(values, q_tensor)
            q1, med, q3 = q_vals[0].item(), q_vals[1].item(), q_vals[2].item()
            # Ensure IQR is never zero
            iqr = max(q3 - q1, self.eps)
            stats.update({"median": med, "iqr": iqr})
        
        elif method == "log-min-max":
            log_vals = torch.log10(torch.clamp(values, min=self.eps))
            min_v, max_v = log_vals.min().item(), log_vals.max().item()
            # Ensure range is never zero
            stats.update({"min": min_v, "max": max(max_v, min_v + self.eps)})
        
        elif method == "symlog":
            percentile = self.norm_config.get(
                "symlog_percentile", DEFAULT_SYMLOG_PERCENTILE
            )
            thr = _robust_quantile(torch.abs(values), percentile).item()
            # Ensure threshold is reasonable
            thr = max(thr, self.eps * 100)
            
            # Apply symlog transformation
            abs_v = torch.abs(values)
            mask = abs_v > thr
            transformed = torch.zeros_like(values)
            
            # Safe division with larger threshold
            transformed[mask] = torch.sign(values[mask]) * (
                torch.log10(abs_v[mask] / thr) + 1
            )
            transformed[~mask] = values[~mask] / thr
            
            # Ensure scale factor is never zero
            sf = transformed.abs().max().item() if transformed.numel() > 0 else 1.0
            stats.update({"threshold": thr, "scale_factor": max(sf, 1.0)})
        
        return stats
    
    @staticmethod
    def normalize_tensor(x: Tensor, method: str, stats: Dict[str, Any]) -> Tensor:
        """
        Apply normalization to a tensor.
        
        Args:
            x: Input tensor
            method: Normalization method
            stats: Statistics dictionary for the method
            
        Returns:
            Normalized tensor
        """
        x = x.to(DTYPE)
        
        if method in ("none", "bool") or not stats:
            return x
        
        eps = stats.get("epsilon", DEFAULT_EPSILON)
        result = x
        
        try:
            if method == "standard":
                result = (x - stats["mean"]) / stats["std"]
            
            elif method == "log-standard":
                x_safe = torch.log10(torch.clamp(x, min=eps))
                result = (x_safe - stats["log_mean"]) / stats["log_std"]
            
            elif method == "signed-log":
                y = torch.sign(x) * torch.log10(torch.abs(x) + 1.0)
                result = (y - stats["mean"]) / stats["std"]
            
            elif method == "log-min-max":
                log_x = torch.log10(torch.clamp(x, min=eps))
                denom = stats["max"] - stats["min"]
                if denom <= 0:
                    raise ValueError("log-min-max stats have zero range")
                normed = (log_x - stats["min"]) / denom
                result = torch.clamp(normed, 0.0, 1.0)
            
            elif method == "max-out":
                result = x / stats["max_val"]
            
            elif method == "iqr":
                result = (x - stats["median"]) / stats["iqr"]
            
            elif method == "scaled_signed_offset_log":
                y = torch.sign(x) * torch.log10(torch.abs(x) + 1)
                result = y / stats["m"]
            
            elif method == "symlog":
                thr, sf = stats["threshold"], stats["scale_factor"]
                abs_x = torch.abs(x)
                linear_mask = abs_x <= thr
                y = torch.zeros_like(x)
                y[linear_mask] = x[linear_mask] / thr
                y[~linear_mask] = torch.sign(x[~linear_mask]) * (
                    torch.log10(abs_x[~linear_mask] / thr) + 1.0
                )
                result = y / sf
            
            else:
                logger.warning(f"Unsupported method '{method}'. Returning raw tensor.")
        
        except KeyError as e:
            logger.error(f"Missing stat '{e}' for '{method}'. Returning raw tensor.")
            return x
        
        # Clamp unbounded normalizations
        if method in ("standard", "log-standard", "signed-log", "iqr"):
            result = torch.clamp(
                result, -NORMALIZED_VALUE_CLAMP, NORMALIZED_VALUE_CLAMP
            )
        
        return result
    
    @staticmethod
    def normalize_array(x: np.ndarray, method: str, stats: Dict[str, Any]) -> None:
        """
        Apply normalization to a numpy array in-place.
        
        Args:
            x: Input array (modified in-place)
            method: Normalization method
            stats: Statistics dictionary for the method
        """
        if method in ("none", "bool") or not stats:
            return
        
        eps = stats.get("epsilon", DEFAULT_EPSILON)
        
        try:
            if method == "standard":
                x[:] = (x - stats["mean"]) / stats["std"]
            
            elif method == "log-standard":
                x_safe = np.log10(np.maximum(x, eps))
                x[:] = (x_safe - stats["log_mean"]) / stats["log_std"]
            
            elif method == "signed-log":
                y = np.sign(x) * np.log10(np.abs(x) + 1.0)
                x[:] = (y - stats["mean"]) / stats["std"]
            
            elif method == "log-min-max":
                log_x = np.log10(np.maximum(x, eps))
                denom = stats["max"] - stats["min"]
                if denom <= 0:
                    raise ValueError("log-min-max stats have zero range")
                normed = (log_x - stats["min"]) / denom
                x[:] = np.clip(normed, 0.0, 1.0)
            
            elif method == "max-out":
                x[:] = x / stats["max_val"]
            
            elif method == "iqr":
                x[:] = (x - stats["median"]) / stats["iqr"]
            
            elif method == "scaled_signed_offset_log":
                y = np.sign(x) * np.log10(np.abs(x) + 1)
                x[:] = y / stats["m"]
            
            elif method == "symlog":
                thr, sf = stats["threshold"], stats["scale_factor"]
                abs_x = np.abs(x)
                linear_mask = abs_x <= thr
                y = np.zeros_like(x)
                y[linear_mask] = x[linear_mask] / thr
                y[~linear_mask] = np.sign(x[~linear_mask]) * (
                    np.log10(abs_x[~linear_mask] / thr) + 1.0
                )
                x[:] = y / sf
            
            else:
                logger.warning(f"Unsupported method '{method}'. Array unchanged.")
        
        except KeyError as e:
            logger.error(f"Missing stat '{e}' for '{method}'. Array unchanged.")
        
        # Clamp unbounded normalizations
        if method in ("standard", "log-standard", "signed-log", "iqr"):
            np.clip(x, -NORMALIZED_VALUE_CLAMP, NORMALIZED_VALUE_CLAMP, out=x)
    
    @staticmethod
    def denormalize_tensor(x: Tensor, method: str, stats: Dict[str, Any]) -> Tensor:
        """
        Reverse normalization on a tensor.
        
        Args:
            x: Normalized tensor
            method: Normalization method used
            stats: Statistics dictionary for the method
            
        Returns:
            Denormalized tensor
        """
        x = x.to(DTYPE)
        
        if method in ("none", "bool"):
            return x
        
        if not stats:
            raise ValueError(f"No stats for denormalization with method '{method}'")
        
        dtype, device = x.dtype, x.device
        eps = stats.get("epsilon", DEFAULT_EPSILON)
        
        def to_t(val: float) -> Tensor:
            """Convert scalar to tensor with correct dtype/device."""
            return torch.as_tensor(val, dtype=dtype, device=device)
        
        if method == "standard":
            return x.mul(to_t(stats["std"])).add(to_t(stats["mean"]))
        
        elif method == "log-standard":
            return 10 ** (x.mul(to_t(stats["log_std"])).add(to_t(stats["log_mean"])))
        
        elif method == "signed-log":
            unscaled_log = x.mul(to_t(stats["std"])).add(to_t(stats["mean"]))
            return torch.sign(unscaled_log) * (10 ** torch.abs(unscaled_log) - 1.0)
        
        elif method == "log-min-max":
            unscaled = (
                torch.clamp(x, 0, 1)
                .mul(to_t(stats["max"] - stats["min"]))
                .add(to_t(stats["min"]))
            )
            return 10**unscaled
        
        elif method == "max-out":
            return x.mul(to_t(stats["max_val"]))
        
        elif method == "iqr":
            return x.mul(to_t(stats["iqr"])).add(to_t(stats["median"]))
        
        elif method == "scaled_signed_offset_log":
            ytmp = x.mul(to_t(stats["m"]))
            return torch.sign(ytmp) * (10 ** torch.abs(ytmp) - 1)
        
        elif method == "symlog":
            unscaled = x.mul(to_t(stats["scale_factor"]))
            abs_unscaled = torch.abs(unscaled)
            linear_mask = abs_unscaled <= 1.0
            thr = to_t(stats["threshold"])
            y = torch.zeros_like(x)
            y[linear_mask] = unscaled[linear_mask].mul(thr)
            y[~linear_mask] = (
                torch.sign(unscaled[~linear_mask])
                * thr
                * (10 ** (abs_unscaled[~linear_mask] - 1.0))
            )
            return y
        
        else:
            raise ValueError(f"Unsupported denormalization method '{method}'")
    
    @staticmethod
    def denormalize(
        v: Union[Tensor, List, float, bool, None],
        metadata: Dict[str, Any],
        var_name: str,
    ) -> Union[Tensor, List, float, bool, None]:
        """
        Denormalize a value using metadata.
        
        Args:
            v: Value to denormalize (tensor, list, scalar, or None)
            metadata: Normalization metadata dictionary
            var_name: Variable name to look up in metadata
            
        Returns:
            Denormalized value in same format as input
        """
        if v is None:
            return None
        
        method = metadata["normalization_methods"].get(var_name, "none")
        if method in ("none", "bool"):
            return v
        
        stats = metadata["per_key_stats"].get(var_name)
        if not stats:
            raise ValueError(f"No stats for '{var_name}' in metadata.")
        
        # Track input type for output conversion
        is_scalar = not isinstance(v, (torch.Tensor, list))
        is_list = isinstance(v, list)
        
        # Convert to tensor for processing
        tensor_v = (
            torch.as_tensor(v, dtype=DTYPE)
            if not isinstance(v, torch.Tensor)
            else v.to(DTYPE)
        )
        
        # Apply denormalization
        denorm_tensor = DataNormalizer.denormalize_tensor(tensor_v, method, stats)
        
        # Convert back to original type
        if is_scalar:
            return denorm_tensor.item()
        elif is_list:
            return denorm_tensor.tolist()
        else:
            return denorm_tensor


__all__ = ["DataNormalizer"]

===== /Users/imalsky/Desktop/Problemulator/src/main.py =====
#!/usr/bin/env python3
"""
main.py - Entry point with separate normalize, train, and tune commands.

Commands:
- normalize: Preprocess and normalize the data
- train: Train a model with current configuration
- tune: Run hyperparameter optimization with Optuna
"""
from __future__ import annotations

import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional

import torch
from torch.profiler import profile, ProfilerActivity, schedule

from dataset import create_collate_fn
from hardware import setup_device
from preprocess import preprocess_data
from train import ModelTrainer
from utils import (
    DEFAULT_SEED,
    PADDING_VALUE,
    ensure_dirs,
    get_config_str,
    load_config,
    load_or_generate_splits,
    save_json,
    seed_everything,
    setup_logging,
)
from hyperparam_search import run_optuna

# Prevent MKL library conflicts
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Default paths
DEFAULT_CONFIG_PATH = Path("config/config.jsonc")
DEFAULT_DATA_DIR = Path("data")
DEFAULT_PROCESSED_DIR = DEFAULT_DATA_DIR / "processed"
DEFAULT_RAW_DIR = DEFAULT_DATA_DIR / "raw"
DEFAULT_MODELS_DIR = Path("models")

logger = logging.getLogger(__name__)


def _get_raw_hdf5_paths(config: Dict[str, Any], raw_dir: Path) -> List[Path]:
    """
    Get list of raw HDF5 file paths from configuration.
    
    Args:
        config: Configuration dictionary
        raw_dir: Directory containing raw HDF5 files
        
    Returns:
        List of HDF5 file paths
        
    Raises:
        ValueError: If no files specified
        FileNotFoundError: If files are missing
    """
    h5_filenames = config.get("data_paths_config", {}).get("hdf5_dataset_filename", [])
    
    if not isinstance(h5_filenames, list) or not h5_filenames:
        raise ValueError("No raw HDF5 files specified in config.")
    
    raw_paths = [raw_dir / fname for fname in h5_filenames]
    missing = [p for p in raw_paths if not p.is_file()]
    
    if missing:
        raise FileNotFoundError(f"Missing raw HDF5 files: {missing}")
    
    return raw_paths


def _parse_arguments() -> argparse.Namespace:
    """Parse command line arguments with subcommands."""
    parser = argparse.ArgumentParser(
        description="Atmospheric profile transformer pipeline.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    
    # Add subcommands
    subparsers = parser.add_subparsers(dest='command', help='Command to run')
    
    # Common arguments for all commands
    parent_parser = argparse.ArgumentParser(add_help=False)
    parent_parser.add_argument(
        "--config",
        type=Path,
        default=DEFAULT_CONFIG_PATH,
        help="Path to configuration file.",
    )
    parent_parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Root data directory (contains raw/ and processed/).",
    )
    parent_parser.add_argument(
        "--models-dir",
        type=Path,
        default=DEFAULT_MODELS_DIR,
        help="Directory for saved models.",
    )
    
    # Normalize command
    normalize_parser = subparsers.add_parser(
        'normalize',
        parents=[parent_parser],
        help='Preprocess and normalize the data'
    )
    normalize_parser.add_argument(
        "--force",
        action="store_true",
        help="Force re-normalization even if cached data exists.",
    )
    
    # Train command
    train_parser = subparsers.add_parser(
        'train',
        parents=[parent_parser],
        help='Train a model with current config'
    )
    train_parser.add_argument(
        "--profile",
        action="store_true",
        help="Enable PyTorch profiler for performance analysis.",
    )
    train_parser.add_argument(
        "--profile-wait",
        type=int,
        default=1,
        help="Number of steps to wait before profiling.",
    )
    train_parser.add_argument(
        "--profile-warmup",
        type=int,
        default=1,
        help="Number of warmup steps for profiler.",
    )
    train_parser.add_argument(
        "--profile-active",
        type=int,
        default=3,
        help="Number of steps to actively profile.",
    )
    train_parser.add_argument(
        "--profile-epochs",
        type=int,
        default=1,
        help="Number of epochs to profile (only used with --profile).",
    )
    
    # Tune command (hyperparameter search)
    tune_parser = subparsers.add_parser(
        'tune',
        parents=[parent_parser],
        help='Run hyperparameter optimization with Optuna'
    )
    tune_parser.add_argument(
        "--num-trials",
        type=int,
        default=50,
        help="Number of Optuna trials for hyperparameter optimization.",
    )
    tune_parser.add_argument(
        "--optuna-study-name",
        type=str,
        default=None,
        help="Optuna study name.",
    )
    tune_parser.add_argument(
        "--resume",
        action="store_true",
        help="Resume an existing Optuna study.",
    )
    
    args = parser.parse_args()
    
    # Check that a command was specified
    if args.command is None:
        parser.print_help()
        sys.exit(1)
    
    return args


def run_normalize(
    config: Dict[str, Any],
    raw_hdf5_paths: List[Path],
    processed_dir: Path,
    model_save_dir: Path,
    force: bool = False,
) -> Dict[str, List[Tuple[str, int]]]:
    """
    Run data normalization step.
    
    Args:
        config: Configuration dictionary
        raw_hdf5_paths: List of raw HDF5 files
        processed_dir: Directory for processed data
        model_save_dir: Directory for saving splits info
        force: If True, force reprocessing
        
    Returns:
        Dictionary of data splits
    """
    logger.info("=== Running Data Normalization ===")
    
    # Load or generate splits
    splits, splits_path = load_or_generate_splits(
        config, processed_dir.parent, raw_hdf5_paths, model_save_dir
    )
    
    # Save splits info
    save_json(
        {"splits_file": str(splits_path.resolve())},
        model_save_dir / "splits_info.json",
    )
    
    # Force reprocessing if requested
    if force:
        logger.info("Force flag set - removing existing processed data")
        import shutil
        if processed_dir.exists():
            shutil.rmtree(processed_dir)
    
    # Run preprocessing
    success = preprocess_data(
        config=config,
        raw_hdf5_paths=raw_hdf5_paths,
        splits=splits,
        processed_dir=processed_dir,
    )
    
    if not success:
        raise RuntimeError("Preprocessing failed due to invalid data.")
    
    logger.info("=== Data Normalization Complete ===")
    return splits


def run_training_with_profiler(
    config: Dict[str, Any],
    device: torch.device,
    model_save_dir: Path,
    processed_dir: Path,
    splits: Dict[str, List[Tuple[str, int]]],
    padding_val: float,
    profile_config: Dict[str, int],
) -> None:
    """
    Run training with PyTorch profiler enabled.
    
    Args:
        config: Configuration dictionary
        device: Compute device
        model_save_dir: Directory for saving models
        processed_dir: Directory with processed data
        splits: Data splits
        padding_val: Padding value
        profile_config: Profiler configuration
    """
    # Temporarily reduce epochs for profiling
    original_epochs = config["training_hyperparameters"]["epochs"]
    config["training_hyperparameters"]["epochs"] = profile_config["epochs"]
    
    logger.info("Starting training with PyTorch profiler...")
    logger.info(f"Profiling {profile_config['epochs']} epoch(s)")
    logger.info(
        f"Profile schedule: wait={profile_config['wait']}, "
        f"warmup={profile_config['warmup']}, active={profile_config['active']}"
    )
    
    # Create profiler schedule
    prof_schedule = schedule(
        wait=profile_config["wait"],
        warmup=profile_config["warmup"],
        active=profile_config["active"],
        repeat=1
    )
    
    # Profile trace directory
    trace_dir = model_save_dir / "profiler_traces"
    ensure_dirs(trace_dir)
    
    # Run training with profiler
    with profile(
        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
        schedule=prof_schedule,
        on_trace_ready=torch.profiler.tensorboard_trace_handler(str(trace_dir)),
        record_shapes=True,
        profile_memory=True,
        with_stack=True,
        with_modules=True
    ) as prof:
        
        collate_fn = create_collate_fn(padding_val)
        trainer = ModelTrainer(
            config=config,
            device=device,
            save_dir=model_save_dir,
            processed_dir=processed_dir,
            splits=splits,
            collate_fn=collate_fn,
        )
        
        # Wrap epoch runner with profiler step
        original_run_epoch = trainer._run_epoch
        
        def profiled_run_epoch(loader, is_train):
            result = original_run_epoch(loader, is_train)
            prof.step()  # Advance profiler
            return result
        
        trainer._run_epoch = profiled_run_epoch
        
        try:
            trainer.train()
        finally:
            trainer._run_epoch = original_run_epoch
    
    # Export profiler results
    chrome_trace_path = trace_dir / "chrome_trace.json"
    prof.export_chrome_trace(str(chrome_trace_path))
    logger.info(f"Chrome trace saved to: {chrome_trace_path}")
    
    # Print profiler summary
    logger.info("\n=== Profiler Summary ===")
    logger.info(prof.key_averages().table(sort_by="cuda_time_total", row_limit=20))
    
    # Export detailed stats
    stats_path = trace_dir / "profiler_stats.txt"
    with open(stats_path, "w") as f:
        f.write(prof.key_averages().table(sort_by="cuda_time_total"))
    logger.info(f"Detailed stats saved to: {stats_path}")
    
    # Restore original epochs
    config["training_hyperparameters"]["epochs"] = original_epochs
    
    logger.info("\nProfiling complete! View results with:")
    logger.info(f"  tensorboard --logdir={trace_dir}")
    logger.info(f"  chrome://tracing (load {chrome_trace_path})")


def run_train(
    config: Dict[str, Any],
    device: torch.device,
    model_save_dir: Path,
    processed_dir: Path,
    raw_hdf5_paths: List[Path],
    args: argparse.Namespace,
) -> None:
    """
    Run the training pipeline.
    
    Args:
        config: Configuration dictionary
        device: Compute device
        model_save_dir: Directory for saving models
        processed_dir: Directory with processed data
        raw_hdf5_paths: List of raw HDF5 files
        args: Command line arguments
    """
    logger.info("=== Running Model Training ===")
    
    # Ensure data is preprocessed
    splits = run_normalize(
        config, raw_hdf5_paths, processed_dir, model_save_dir, force=False
    )
    
    # Get padding value
    padding_val = float(
        config.get("data_specification", {}).get("padding_value", PADDING_VALUE)
    )
    
    # Run with profiler if requested
    if args.profile:
        profile_config = {
            "wait": args.profile_wait,
            "warmup": args.profile_warmup,
            "active": args.profile_active,
            "epochs": args.profile_epochs,
        }
        run_training_with_profiler(
            config, device, model_save_dir, processed_dir,
            splits, padding_val, profile_config
        )
    else:
        # Normal training
        collate_fn = create_collate_fn(padding_val)
        trainer = ModelTrainer(
            config=config,
            device=device,
            save_dir=model_save_dir,
            processed_dir=processed_dir,
            splits=splits,
            collate_fn=collate_fn,
        )
        trainer.train()
        trainer.test()
    
    logger.info("=== Model Training Complete ===")


def run_tune(
    config: Dict[str, Any],
    device: torch.device,
    model_save_dir: Path,
    processed_dir: Path,
    raw_hdf5_paths: List[Path],
    args: argparse.Namespace,
) -> None:
    """
    Run hyperparameter tuning.
    
    Args:
        config: Configuration dictionary
        device: Compute device
        model_save_dir: Directory for saving models
        processed_dir: Directory with processed data
        raw_hdf5_paths: List of raw HDF5 files
        args: Command line arguments
    """
    logger.info("=== Running Hyperparameter Tuning ===")
    
    # Ensure data is preprocessed
    splits = run_normalize(
        config, raw_hdf5_paths, processed_dir, model_save_dir, force=False
    )
    
    # Get padding value
    padding_val = float(
        config.get("data_specification", {}).get("padding_value", PADDING_VALUE)
    )
    
    # Create subdirectory for hyperparameter search
    hyperparam_dir = model_save_dir / "hyperparam_search"
    ensure_dirs(hyperparam_dir)
    
    # Run Optuna optimization
    run_optuna(
        config, args, device, processed_dir, splits, padding_val, hyperparam_dir
    )
    
    logger.info("=== Hyperparameter Tuning Complete ===")


def main() -> int:
    """
    Main entry point.
    
    Returns:
        Exit code (0 for success, non-zero for failure)
    """
    args = _parse_arguments()
    
    # Setup directories
    ensure_dirs(args.data_dir, args.models_dir)
    setup_logging()
    
    try:
        logger.info(f"Running command: {args.command}")
        logger.info(f"Using config: {args.config.resolve()}")
        
        # Load configuration
        config = load_config(args.config)
        
        # Set random seed for reproducibility
        seed = config.get("miscellaneous_settings", {}).get("random_seed", DEFAULT_SEED)
        seed_everything(seed)
        
        # Setup compute device
        device = setup_device()
        
        # Apply hardware-specific settings
        if device.type == "cuda":
            if torch.cuda.get_device_capability()[0] >= 8:
                torch.set_float32_matmul_precision("high")
                logger.info("Set float32 matmul precision to 'high' for A100/newer GPU")
        
        # Setup paths
        raw_dir = args.data_dir / "raw"
        processed_dir = args.data_dir / "processed"
        raw_hdf5_paths = _get_raw_hdf5_paths(config, raw_dir)
        
        # Create model save directory
        model_folder = get_config_str(
            config, "output_paths_config", "fixed_model_foldername", "model training"
        )
        model_save_dir = args.models_dir / model_folder
        ensure_dirs(model_save_dir)
        
        # Setup logging to file
        log_file = model_save_dir / f"{args.command}_run.log"
        setup_logging(log_file=log_file, force=True)
        
        # Save configuration
        save_json(config, model_save_dir / f"{args.command}_config.json")
        
        # Execute the appropriate command
        if args.command == 'normalize':
            run_normalize(
                config, raw_hdf5_paths, processed_dir, model_save_dir,
                force=args.force
            )
        
        elif args.command == 'train':
            run_train(
                config, device, model_save_dir, processed_dir,
                raw_hdf5_paths, args
            )
        
        elif args.command == 'tune':
            run_tune(
                config, device, model_save_dir, processed_dir,
                raw_hdf5_paths, args
            )
        
        else:
            raise ValueError(f"Unknown command: {args.command}")
        
        logger.info(f"{args.command.capitalize()} completed successfully.")
        return 0
        
    except RuntimeError as e:
        logger.error(f"Pipeline error: {e}", exc_info=False)
        return 1
        
    except KeyboardInterrupt:
        logger.warning("Interrupted by user (Ctrl+C).")
        return 130
        
    except Exception as e:
        logger.critical(f"Unhandled exception: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())

===== /Users/imalsky/Desktop/Problemulator/src/test_export.py =====
#!/usr/bin/env python3
"""
test_export.py - Test script to verify model export functionality.

This script:
1. Creates a model from configuration
2. Tests forward pass with dummy data
3. Exports the model using torch.export
4. Validates exported model produces identical results
5. Tests with multiple batch sizes
"""
import torch
from pathlib import Path
from typing import Dict, Optional

from model import create_prediction_model, export_model
from utils import load_config


def test_model_export(
    config_path: str = "../config/config.jsonc",
    export_dir: Optional[Path] = None,
    test_batch_sizes: list = None,
) -> None:
    """
    Test that model can be successfully exported and produces identical results.
    
    Args:
        config_path: Path to configuration file
        export_dir: Directory for export test (defaults to ./test_export)
        test_batch_sizes: List of batch sizes to test (defaults to [1, 2, 8, 16])
    """
    print("=" * 60)
    print("Model Export Test")
    print("=" * 60)
    
    # Load configuration
    print(f"\n1. Loading config from: {config_path}")
    config = load_config(config_path)
    
    # Setup device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   Using device: {device}")
    
    # Create model
    print("\n2. Creating model...")
    model = create_prediction_model(config, device=device, compile_model=False)
    
    # Extract dimensions from config
    data_spec = config["data_specification"]
    model_params = config["model_hyperparameters"]
    
    input_dim = len(data_spec["input_variables"])
    global_dim = len(data_spec.get("global_variables", []))
    seq_length = model_params["max_sequence_length"]
    
    print(f"   Input dimension: {input_dim}")
    print(f"   Global dimension: {global_dim}")
    print(f"   Sequence length: {seq_length}")
    
    # Create dummy inputs
    print("\n3. Creating dummy inputs...")
    batch_size = 4
    
    sequence = torch.randn(batch_size, seq_length, input_dim, device=device)
    global_features = (
        torch.randn(batch_size, global_dim, device=device)
        if global_dim > 0 else None
    )
    
    # Create padding mask (True = padding position)
    sequence_mask = torch.zeros(batch_size, seq_length, dtype=torch.bool, device=device)
    # Add some padding to last sequences for testing
    sequence_mask[0, 50:] = True
    sequence_mask[1, 55:] = True
    
    print(f"   Sequence shape: {sequence.shape}")
    if global_features is not None:
        print(f"   Global features shape: {global_features.shape}")
    print(f"   Padding mask shape: {sequence_mask.shape}")
    
    # Test forward pass
    print("\n4. Testing forward pass...")
    with torch.no_grad():
        output = model(sequence, global_features, sequence_mask)
    
    print(f"   ✓ Model forward pass successful")
    print(f"   Output shape: {output.shape}")
    
    # Prepare export directory
    if export_dir is None:
        export_dir = Path("test_export")
    export_dir.mkdir(exist_ok=True)
    
    # Test export
    print("\n5. Testing model export...")
    example_input = {
        "sequence": sequence,
        "global_features": global_features,
        "sequence_mask": sequence_mask,
    }
    
    export_path = export_dir / "test_model"
    
    try:
        # Export the model
        export_model(model, example_input, export_path, config)
        print("   ✓ Model export successful!")
        
        # Load and test exported model
        exported_path = export_dir / "test_model_exported.pt2"
        
        if exported_path.exists():
            print("\n6. Loading exported model...")
            exported_prog = torch.export.load(str(exported_path))
            print("   ✓ Exported model loaded successfully")
            
            # Test with original batch size
            print("\n7. Validating exported model output...")
            with torch.no_grad():
                # Move to CPU for export testing
                test_kwargs = {
                    "sequence": sequence.cpu(),
                }
                if global_features is not None:
                    test_kwargs["global_features"] = global_features.cpu()
                if sequence_mask is not None:
                    test_kwargs["sequence_mask"] = sequence_mask.cpu()
                
                # Compare outputs
                model_cpu = model.to('cpu')
                original_output = model_cpu(**test_kwargs)
                exported_output = exported_prog.module()(**test_kwargs)
                
                max_diff = torch.max(torch.abs(original_output - exported_output)).item()
                
                if torch.allclose(original_output, exported_output, rtol=1e-4, atol=1e-5):
                    print(f"   ✓ Outputs match! Maximum difference: {max_diff:.2e}")
                else:
                    print(f"   ⚠ Outputs differ! Maximum difference: {max_diff:.2e}")
            
            # Test with different batch sizes
            print("\n8. Testing dynamic batch sizes...")
            
            if test_batch_sizes is None:
                test_batch_sizes = [1, 2, 8, 16]
            
            for test_batch_size in test_batch_sizes:
                try:
                    # Create test inputs
                    test_seq = torch.randn(test_batch_size, seq_length, input_dim)
                    test_global = (
                        torch.randn(test_batch_size, global_dim)
                        if global_dim > 0 else None
                    )
                    test_mask = torch.zeros(test_batch_size, seq_length, dtype=torch.bool)
                    
                    # Prepare kwargs
                    test_kwargs = {"sequence": test_seq}
                    if test_global is not None:
                        test_kwargs["global_features"] = test_global
                    if test_mask is not None:
                        test_kwargs["sequence_mask"] = test_mask
                    
                    # Run exported model
                    with torch.no_grad():
                        exported_out = exported_prog.module()(**test_kwargs)
                    
                    print(f"   ✓ Batch size {test_batch_size:2d}: "
                          f"Output shape {tuple(exported_out.shape)}")
                    
                except Exception as e:
                    print(f"   ✗ Batch size {test_batch_size:2d}: Failed - {e}")
            
            # Test static export if it exists
            static_path = export_dir / "test_model_exported_static.pt2"
            if static_path.exists():
                print("\n9. Static export found, testing...")
                static_prog = torch.export.load(str(static_path))
                
                # Test with original batch size only (static shapes)
                with torch.no_grad():
                    test_kwargs = {
                        "sequence": sequence.cpu(),
                    }
                    if global_features is not None:
                        test_kwargs["global_features"] = global_features.cpu()
                    if sequence_mask is not None:
                        test_kwargs["sequence_mask"] = sequence_mask.cpu()
                    
                    static_output = static_prog.module()(**test_kwargs)
                    print(f"   ✓ Static model works with original batch size {batch_size}")
        
        else:
            print(f"   ⚠ Exported model not found at {exported_path}")
    
    except Exception as e:
        print(f"   ✗ Export failed: {e}")
        raise
    
    # Summary
    print("\n" + "=" * 60)
    print("Export Test Summary")
    print("=" * 60)
    print("✓ Model creation successful")
    print("✓ Forward pass successful")
    print("✓ Model export successful")
    print("✓ Exported model validation passed")
    print("✓ Dynamic batch size support confirmed")
    print("\nModel is export-compatible and ready for deployment!")


def test_export_with_real_data(
    config_path: str = "../config/config.jsonc",
    checkpoint_path: Optional[str] = None,
) -> None:
    """
    Test export with a trained model checkpoint.
    
    Args:
        config_path: Path to configuration file
        checkpoint_path: Path to trained model checkpoint
    """
    print("=" * 60)
    print("Model Export Test with Trained Weights")
    print("=" * 60)
    
    # Load configuration
    config = load_config(config_path)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Create model
    model = create_prediction_model(config, device=device, compile_model=False)
    
    # Load checkpoint if provided
    if checkpoint_path and Path(checkpoint_path).exists():
        print(f"\nLoading checkpoint from: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        state_dict = checkpoint["state_dict"]
        # Handle compiled model state dict
        if any(k.startswith("_orig_mod.") for k in state_dict):
            state_dict = {
                k.replace("_orig_mod.", ""): v
                for k, v in state_dict.items()
            }
        
        model.load_state_dict(state_dict)
        print(f"   Loaded weights from epoch {checkpoint.get('epoch', 'unknown')}")
        print(f"   Validation loss: {checkpoint.get('val_loss', 'N/A')}")
    
    # Run standard export test
    test_model_export(config_path=config_path)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Test model export functionality")
    parser.add_argument(
        "--config",
        type=str,
        default="../config/config.jsonc",
        help="Path to configuration file",
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        default=None,
        help="Optional path to model checkpoint",
    )
    parser.add_argument(
        "--export-dir",
        type=Path,
        default=None,
        help="Directory for export test (default: ./test_export)",
    )
    parser.add_argument(
        "--batch-sizes",
        type=int,
        nargs="+",
        default=None,
        help="Batch sizes to test (default: 1 2 8 16)",
    )
    
    args = parser.parse_args()
    
    if args.checkpoint:
        # Test with trained model
        test_export_with_real_data(
            config_path=args.config,
            checkpoint_path=args.checkpoint,
        )
    else:
        # Test with random weights
        test_model_export(
            config_path=args.config,
            export_dir=args.export_dir,
            test_batch_sizes=args.batch_sizes,
        )

===== /Users/imalsky/Desktop/Problemulator/src/hyperparam_search.py =====
#!/usr/bin/env python3
"""
hyperparam_search.py - Optuna hyperparameter optimization with aggressive pruning.

This module orchestrates hyperparameter optimization using Optuna with:
- Efficient data reuse (preprocessed data loaded once)
- Aggressive early stopping for bad trials
- Reduced configurations for faster exploration
"""
from __future__ import annotations

import logging
from argparse import Namespace
from copy import deepcopy
from pathlib import Path
from typing import Any, Dict, List, Tuple, Optional

import optuna
from optuna.pruners import MedianPruner, PercentilePruner
from optuna.samplers import TPESampler
import torch

from dataset import create_collate_fn
from train import ModelTrainer
from utils import ensure_dirs, save_json, setup_logging

logger = logging.getLogger(__name__)


class AggressivePruner(optuna.pruners.BasePruner):
    """
    Custom aggressive pruner combining multiple strategies.
    
    Prunes if ANY of the following conditions are met:
    1. Current loss is worse than median of previous trials at same step
    2. Current loss is in the worst 25th percentile
    3. Loss hasn't improved for patience steps
    """
    
    def __init__(
        self,
        n_startup_trials: int = 5,
        n_warmup_steps: int = 3,
        patience: int = 3,
        min_improvement: float = 0.001,
        percentile: float = 75.0,  # Prune bottom 25%
    ):
        """
        Initialize aggressive pruner.
        
        Args:
            n_startup_trials: Number of trials before pruning starts
            n_warmup_steps: Number of steps before pruning in each trial
            patience: Steps without improvement before pruning
            min_improvement: Minimum improvement to reset patience
            percentile: Keep top percentile of trials
        """
        self._median_pruner = MedianPruner(
            n_startup_trials=n_startup_trials,
            n_warmup_steps=n_warmup_steps,
        )
        self._percentile_pruner = PercentilePruner(
            percentile=percentile,
            n_startup_trials=n_startup_trials,
            n_warmup_steps=n_warmup_steps,
        )
        self.patience = patience
        self.min_improvement = min_improvement
        self._trial_best_values: Dict[int, Tuple[float, int]] = {}
    
    def prune(self, study: optuna.Study, trial: optuna.FrozenTrial) -> bool:
        """
        Determine if trial should be pruned.
        
        Args:
            study: Optuna study
            trial: Current trial
            
        Returns:
            True if trial should be pruned
        """
        # Check median pruner
        if self._median_pruner.prune(study, trial):
            logger.info(f"Trial {trial.number} pruned by median pruner")
            return True
        
        # Check percentile pruner
        if self._percentile_pruner.prune(study, trial):
            logger.info(f"Trial {trial.number} pruned by percentile pruner")
            return True
        
        # Check patience-based pruning
        step = trial.last_step
        if step is None:
            return False
        
        current_value = trial.intermediate_values[step]
        trial_id = trial.number
        
        # Track best value for this trial
        if trial_id not in self._trial_best_values:
            self._trial_best_values[trial_id] = (current_value, step)
        else:
            best_value, best_step = self._trial_best_values[trial_id]
            
            # Check for improvement
            if current_value < best_value - self.min_improvement:
                # Improvement found, update best
                self._trial_best_values[trial_id] = (current_value, step)
            elif step - best_step >= self.patience:
                # No improvement for patience steps
                logger.info(
                    f"Trial {trial.number} pruned by patience "
                    f"(no improvement for {self.patience} steps)"
                )
                return True
        
        return False


def create_reduced_config(
    config: Dict[str, Any],
    fraction: float = 0.1,
    max_epochs: int = 30,
) -> Dict[str, Any]:
    """
    Create a reduced configuration for faster hyperparameter search.
    
    Args:
        config: Original configuration
        fraction: Fraction of data to use
        max_epochs: Maximum epochs for trials
        
    Returns:
        Modified configuration for faster trials
    """
    reduced_config = deepcopy(config)
    
    # Reduce dataset size
    reduced_config["training_hyperparameters"]["dataset_fraction_to_use"] = fraction
    
    # Reduce epochs
    current_epochs = reduced_config["training_hyperparameters"].get("epochs", 100)
    reduced_config["training_hyperparameters"]["epochs"] = min(max_epochs, current_epochs)
    
    # More aggressive early stopping for trials
    reduced_config["training_hyperparameters"]["early_stopping_patience"] = 5
    reduced_config["training_hyperparameters"]["min_delta"] = 1e-4
    
    # Disable model export during trials to save time
    reduced_config["miscellaneous_settings"]["torch_export"] = False
    
    return reduced_config


def run_optuna(
    config: Dict[str, Any],
    args: Namespace,
    device: torch.device,
    processed_dir: Path,
    splits: Dict[str, List[Tuple[str, int]]],
    padding_val: float,
    model_save_dir: Path,
) -> None:
    """
    Run Optuna hyperparameter search with aggressive pruning.
    
    Args:
        config: Configuration dictionary
        args: Command line arguments
        device: Compute device
        processed_dir: Directory with preprocessed data
        splits: Data splits
        padding_val: Padding value
        model_save_dir: Directory for saving results
    """
    ensure_dirs(model_save_dir)
    
    # Get search space from config
    search_space = config.get("hyperparameter_search", {})
    if not search_space:
        logger.error("'hyperparameter_search' section not found in config.")
        return
    
    # Create reduced config for faster trials
    trial_config_base = create_reduced_config(config, fraction=0.2, max_epochs=50)
    
    logger.info(
        f"Using reduced config for trials: "
        f"{trial_config_base['training_hyperparameters']['dataset_fraction_to_use']:.0%} of data, "
        f"max {trial_config_base['training_hyperparameters']['epochs']} epochs"
    )
    
    def objective(trial: optuna.Trial) -> float:
        """
        Objective function for a single trial.
        
        Args:
            trial: Optuna trial
            
        Returns:
            Best validation loss achieved
        """
        # Copy base config
        trial_config = deepcopy(trial_config_base)
        
        # Model architecture parameters
        d_model = trial.suggest_categorical("d_model", search_space["d_model"])
        trial_config["model_hyperparameters"]["d_model"] = d_model
        
        # Find valid nhead values (must divide d_model)
        possible_nheads = [
            div for div in search_space["nhead_divisors"] if d_model % div == 0
        ]
        if not possible_nheads:
            logger.warning(
                f"For trial {trial.number}, no valid nhead_divisor for d_model={d_model}. "
                f"Defaulting to 1."
            )
            possible_nheads = [1]
        
        nhead = trial.suggest_categorical("nhead", possible_nheads)
        trial_config["model_hyperparameters"]["nhead"] = nhead
        
        # Number of encoder layers
        layers_range = search_space["num_encoder_layers"]
        num_encoder_layers = trial.suggest_int(
            "num_encoder_layers", low=layers_range[0], high=layers_range[1]
        )
        trial_config["model_hyperparameters"]["num_encoder_layers"] = num_encoder_layers
        
        # Feedforward dimension
        dim_feedforward = trial.suggest_categorical(
            "dim_feedforward", search_space["dim_feedforward"]
        )
        trial_config["model_hyperparameters"]["dim_feedforward"] = dim_feedforward
        
        # Regularization
        dropout = trial.suggest_float("dropout", **search_space["dropout"])
        trial_config["model_hyperparameters"]["dropout"] = dropout
        
        # Training parameters
        learning_rate = trial.suggest_float(
            "learning_rate", **search_space["learning_rate"]
        )
        trial_config["training_hyperparameters"]["learning_rate"] = learning_rate
        
        batch_size = trial.suggest_categorical("batch_size", search_space["batch_size"])
        trial_config["training_hyperparameters"]["batch_size"] = batch_size
        
        weight_decay = trial.suggest_float(
            "weight_decay", **search_space["weight_decay"]
        )
        trial_config["training_hyperparameters"]["weight_decay"] = weight_decay
        
        # Create trial directory
        trial_save_dir = model_save_dir / f"trial_{trial.number:04d}"
        ensure_dirs(trial_save_dir)
        
        # Save trial config
        save_json(trial_config, trial_save_dir / "trial_config.json")
        
        # Setup trial-specific logging
        trial_log_file = trial_save_dir / "trial_log.log"
        setup_logging(log_file=trial_log_file, force=True)
        
        logger.info(f"Starting Trial {trial.number} with params: {trial.params}")
        
        # Create collate function
        collate_fn = create_collate_fn(padding_val)
        
        try:
            # Data is already preprocessed and will be reused
            logger.info("Reusing preprocessed data from disk/cache")
            
            # Create trainer
            trainer = ModelTrainer(
                config=trial_config,
                device=device,
                save_dir=trial_save_dir,
                processed_dir=processed_dir,
                splits=splits,
                collate_fn=collate_fn,
                optuna_trial=trial,  # Pass trial for pruning
            )
            
            # Train and get best validation loss
            best_val_loss = trainer.train()
            
            # Save trial results
            trial_results = {
                "trial_number": trial.number,
                "best_val_loss": best_val_loss,
                "params": trial.params,
                "state": trial.state.name,
            }
            save_json(trial_results, trial_save_dir / "trial_results.json")
            
            return best_val_loss
            
        except optuna.exceptions.TrialPruned:
            logger.info(f"Trial {trial.number} was pruned.")
            raise
            
        except Exception as e:
            logger.error(
                f"Trial {trial.number} failed with exception: {e}", exc_info=True
            )
            return float("inf")
    
    # Create study with aggressive pruning
    study_name = args.optuna_study_name or "atmospheric_transformer_study"
    storage_path = f"sqlite:///{model_save_dir / 'optuna_study.db'}"
    
    # Use TPE sampler with more startup trials for better exploration
    sampler_seed = config.get("miscellaneous_settings", {}).get("random_seed", 42)
    sampler = TPESampler(
        seed=sampler_seed,
        n_startup_trials=10,  # More exploration initially
        n_ei_candidates=24,   # More candidates for acquisition function
    )
    
    # Use custom aggressive pruner
    pruner = AggressivePruner(
        n_startup_trials=5,
        n_warmup_steps=3,
        patience=3,
        min_improvement=0.001,
        percentile=75.0,  # Prune bottom 25%
    )
    
    # Create or load study
    if args.resume and Path(storage_path.replace("sqlite:///", "")).exists():
        logger.info(f"Resuming existing study '{study_name}'")
        study = optuna.load_study(
            study_name=study_name,
            storage=storage_path,
            sampler=sampler,
            pruner=pruner,
        )
    else:
        logger.info(f"Creating new study '{study_name}'")
        study = optuna.create_study(
            direction="minimize",
            study_name=study_name,
            storage=storage_path,
            load_if_exists=False,
            sampler=sampler,
            pruner=pruner,
        )
    
    logger.info(
        f"Starting Optuna study '{study_name}' with {args.num_trials} trials."
    )
    logger.info(f"Sampler: {sampler.__class__.__name__}")
    logger.info(f"Pruner: {pruner.__class__.__name__} (aggressive settings)")
    logger.info(f"Results will be saved in: {model_save_dir}")
    
    # Run optimization
    study.optimize(
        objective,
        n_trials=args.num_trials,
        gc_after_trial=True,
        show_progress_bar=True,
    )
    
    # Log results
    logger.info("\n--- Optuna Hyperparameter Search Complete ---")
    logger.info(f"Number of finished trials: {len(study.trials)}")
    
    if study.best_trial:
        logger.info(f"Best trial: Trial #{study.best_trial.number}")
        logger.info(f"  Best Validation Loss: {study.best_value:.6f}")
        logger.info("  Best Hyperparameters:")
        for key, value in study.best_trial.params.items():
            logger.info(f"    {key}: {value}")
        
        # Save best parameters
        best_params_path = model_save_dir / "best_hyperparameters.json"
        results_summary = {
            "best_value": study.best_value,
            "best_params": study.best_trial.params,
            "best_trial_number": study.best_trial.number,
            "n_trials": len(study.trials),
            "n_pruned": len([
                t for t in study.trials
                if t.state == optuna.trial.TrialState.PRUNED
            ]),
            "n_complete": len([
                t for t in study.trials
                if t.state == optuna.trial.TrialState.COMPLETE
            ]),
            "n_failed": len([
                t for t in study.trials
                if t.state == optuna.trial.TrialState.FAIL
            ]),
        }
        save_json(results_summary, best_params_path)
        logger.info(f"Best hyperparameters saved to {best_params_path}")
        
        # Create final config with best params for full training
        final_config = deepcopy(config)
        model_hp = final_config["model_hyperparameters"]
        train_hp = final_config["training_hyperparameters"]
        
        # Apply best parameters
        best_params = study.best_trial.params
        model_hp["d_model"] = best_params["d_model"]
        model_hp["nhead"] = best_params["nhead"]
        model_hp["num_encoder_layers"] = best_params["num_encoder_layers"]
        model_hp["dim_feedforward"] = best_params["dim_feedforward"]
        model_hp["dropout"] = best_params["dropout"]
        train_hp["learning_rate"] = best_params["learning_rate"]
        train_hp["batch_size"] = best_params["batch_size"]
        train_hp["weight_decay"] = best_params["weight_decay"]
        
        save_json(final_config, model_save_dir / "best_config.json")
        logger.info("Created best_config.json for full training run")
        
        # Print summary
        logger.info("\nTo train with best hyperparameters, run:")
        logger.info(f"  python main.py train --config {model_save_dir / 'best_config.json'}")
        
    else:
        logger.warning("No successful trials completed in the study.")
    
    # Print pruning statistics
    pruned_trials = [
        t for t in study.trials
        if t.state == optuna.trial.TrialState.PRUNED
    ]
    
    if pruned_trials:
        logger.info(f"\nPruning statistics:")
        logger.info(f"  Total pruned: {len(pruned_trials)}/{len(study.trials)}")
        logger.info(f"  Pruning rate: {len(pruned_trials)/len(study.trials):.1%}")
        
        # Analyze when trials were pruned
        pruning_steps = []
        for trial in pruned_trials:
            if trial.intermediate_values:
                last_step = max(trial.intermediate_values.keys())
                pruning_steps.append(last_step)
        
        if pruning_steps:
            avg_pruning_step = sum(pruning_steps) / len(pruning_steps)
            logger.info(f"  Average pruning step: {avg_pruning_step:.1f}")


__all__ = ["run_optuna", "AggressivePruner", "create_reduced_config"]

